{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert, Update and Delete data on S3 Datalake using Apache Hudi and Amazon EMR - Demo\n",
    "\n",
    "## Table of Contents:\n",
    "\n",
    "1. [Overview](#Overview)\n",
    "2. [Copy On Write](#Copy-On-Write)<br>\n",
    "    2.1 [Bulk Insert the Initial Dataset](#Bulk-Insert-the-Initial-Dataset)<br>\n",
    "    2.2 [Batch Upsert some records](#Batch-Upsert-some-records)<br>\n",
    "    2.3 [Deleting Records](#Deleting-Records)<br>\n",
    "3. [Rollback](#Rollback)<br>\n",
    "4. [Time travel with Hudi](#Time-travel-with-Hudi) \n",
    "5. [Merge on Read](#Merge-On-Read)<br>\n",
    "    5.1 [Bulk Insert the Initial Dataset](#Bulk-Insert-the-Initial-Dataset)<br>\n",
    "    5.2 [Batch Upsert some records](#Batch-Upsert-some-records)<br>\n",
    "    5.3 [Compaction for MOR tables](#Compaction-for-MOR-tables) <br>\n",
    "6. [Working with Partitioned Tables](#Working-with-Partitioned-Tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates using PySpark on [Apache Hudi](https://aws.amazon.com/emr/features/hudi/) on Amazon EMR to insert/upsert/delete records to an S3 data lake.\n",
    "\n",
    "Here are some good reference links to read later:\n",
    "\n",
    "* [Apache Hudi concepts](https://hudi.apache.org/concepts.html)\n",
    "* [How Hudi Works](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hudi-how-it-works.html)\n",
    "\n",
    "This notebook covers the following concepts when writing Copy-On-Write and Merge-On-Read tables to an S3 Datalake:\n",
    "\n",
    "- Write Hudi Spark jobs in PySpark.\n",
    "- Bulk Insert the Initial Dataset.\n",
    "- Write a MultiKey Partitioned table as well as a Non-Partitioned table.\n",
    "- Tune the Bulk Insert write performance as per expected number of target files.\n",
    "- Sync the Hudi tables to the Hive/Glue Catalog.\n",
    "- Upsert some records to a Hudi table.\n",
    "- Delete from records from a Hudi table.\n",
    "- Understand how Hudi Commit Retention policy works.\n",
    "- Time travel with Hudi incremental tables using incremental and point-in-time queries\n",
    "- Perform Insert/Upsert/Delete operations for Merge-on-Read table and understand the difference between MOR and COW tables\n",
    "\n",
    "\n",
    "#### This demo runs fine on a 1 node (r5.4xlarge) EMR cluster with release label = 5.30 or 5.30.1 or 5.31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by initializing the Spark Session to connect this notebook to our Spark EMR cluster:\n",
    "\n",
    "Note that the files hudi-spark-bundle.jar and spark-avro.jar are copied into HDFS.\n",
    "\n",
    "hadoop fs -copyFromLocal /usr/lib/hudi/hudi-spark-bundle.jar hdfs:///user/hadoop/\n",
    "\n",
    "hadoop fs -copyFromLocal /usr/lib/spark/external/lib/spark-avro.jar hdfs:///user/hadoop/\n",
    "\n",
    "hadoop fs -copyFromLocal /usr/lib/spark/jars/httpclient-4.5.9.jar hdfs:///user/hadoop/\n",
    "\n",
    "#### If you plan to use EMR Release 5.32, then include the following jar in the config as well\n",
    "\n",
    "hadoop fs -copyFromLocal /usr/lib/spark/jars/httpcore-4.4.11.jar hdfs:///user/hadoop/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars': 'hdfs:///user/hadoop/httpclient-4.5.9.jar, hdfs:///user/hadoop/httpcore-4.4.11.jar, hdfs:///user/hadoop/hudi-spark-bundle.jar,hdfs:///user/hadoop/spark-avro.jar', 'spark.sql.hive.convertMetastoreParquet': 'false', 'spark.serializer': 'org.apache.spark.serializer.KryoSerializer', 'spark.dynamicAllocation.executorIdleTimeout': 3600}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":  { \n",
    "             \"spark.jars\":\"hdfs:///user/hadoop/httpclient-4.5.9.jar,hdfs:///user/hadoop/hudi-spark-bundle.jar,hdfs:///user/hadoop/spark-avro.jar\",\n",
    "             \"spark.sql.hive.convertMetastoreParquet\":\"false\",     \n",
    "             \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "             \"spark.dynamicAllocation.executorIdleTimeout\": 3600\n",
    "           } \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Following are the configuration items we will use in this demo\n",
    "\n",
    "Make sure to point the target parameter to your S3 bucket (replace <your-bucket> with an S3 bucket name in your AWS account in the same region as your EMR cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c97a49d5e64dffab54a0af8a6bc10b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>5</td><td>application_1608128907391_0006</td><td>pyspark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## CHANGE ME ##\n",
    "config = {\n",
    "    \"table_name\": \"hudi_trips_table\",\n",
    "    \"target\": \"s3://rr-iad-analytics-demos/emr/hudi-demo1/hudi_trips_table\",\n",
    "    \"primary_key\": \"trip_id\",\n",
    "    \"sort_key\": \"tstamp\",\n",
    "    \"commits_to_retain\": \"10\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constants for Python to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a6faecf3b2f4fbc8a766f462fea9787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# General Constants\n",
    "HUDI_FORMAT = \"org.apache.hudi\"\n",
    "TABLE_NAME = \"hoodie.table.name\"\n",
    "RECORDKEY_FIELD_OPT_KEY = \"hoodie.datasource.write.recordkey.field\"\n",
    "PRECOMBINE_FIELD_OPT_KEY = \"hoodie.datasource.write.precombine.field\"\n",
    "OPERATION_OPT_KEY = \"hoodie.datasource.write.operation\"\n",
    "BULK_INSERT_OPERATION_OPT_VAL = \"bulk_insert\"\n",
    "UPSERT_OPERATION_OPT_VAL = \"upsert\"\n",
    "BULK_INSERT_PARALLELISM = \"hoodie.bulkinsert.shuffle.parallelism\"\n",
    "UPSERT_PARALLELISM = \"hoodie.upsert.shuffle.parallelism\"\n",
    "S3_CONSISTENCY_CHECK = \"hoodie.consistency.check.enabled\"\n",
    "HUDI_CLEANER_POLICY = \"hoodie.cleaner.policy\"\n",
    "KEEP_LATEST_COMMITS = \"KEEP_LATEST_COMMITS\"\n",
    "HUDI_COMMITS_RETAINED = \"hoodie.cleaner.commits.retained\"\n",
    "PAYLOAD_CLASS_OPT_KEY = \"hoodie.datasource.write.payload.class\"\n",
    "EMPTY_PAYLOAD_CLASS_OPT_VAL = \"org.apache.hudi.common.model.EmptyHoodieRecordPayload\"\n",
    "\n",
    "# Hive Constants\n",
    "HIVE_SYNC_ENABLED_OPT_KEY=\"hoodie.datasource.hive_sync.enable\"\n",
    "HIVE_PARTITION_FIELDS_OPT_KEY=\"hoodie.datasource.hive_sync.partition_fields\"\n",
    "HIVE_ASSUME_DATE_PARTITION_OPT_KEY=\"hoodie.datasource.hive_sync.assume_date_partitioning\"\n",
    "HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY=\"hoodie.datasource.hive_sync.partition_extractor_class\"\n",
    "HIVE_TABLE_OPT_KEY=\"hoodie.datasource.hive_sync.table\"\n",
    "\n",
    "# Partition Constants\n",
    "NONPARTITION_EXTRACTOR_CLASS_OPT_VAL=\"org.apache.hudi.hive.NonPartitionedExtractor\"\n",
    "MULTIPART_KEYS_EXTRACTOR_CLASS_OPT_VAL=\"org.apache.hudi.hive.MultiPartKeysValueExtractor\"\n",
    "KEYGENERATOR_CLASS_OPT_KEY=\"hoodie.datasource.write.keygenerator.class\"\n",
    "NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL=\"org.apache.hudi.keygen.NonpartitionedKeyGenerator\"\n",
    "COMPLEX_KEYGENERATOR_CLASS_OPT_VAL=\"org.apache.hudi.ComplexKeyGenerator\"\n",
    "PARTITIONPATH_FIELD_OPT_KEY=\"hoodie.datasource.write.partitionpath.field\"\n",
    "\n",
    "#Incremental Constants\n",
    "VIEW_TYPE_OPT_KEY=\"hoodie.datasource.view.type\"\n",
    "BEGIN_INSTANTTIME_OPT_KEY=\"hoodie.datasource.read.begin.instanttime\"\n",
    "VIEW_TYPE_INCREMENTAL_OPT_VAL=\"incremental\"\n",
    "END_INSTANTTIME_OPT_KEY=\"hoodie.datasource.read.end.instanttime\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to create JSON data and Spark dataframe from this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6e3120a44504401b81450e604a5903f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Generates Data\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def get_json_data(start, count, dest):\n",
    "    time_stamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    data = [{\"trip_id\": i, \"tstamp\": time_stamp, \"route_id\": chr(65 + (i % 10)), \"destination\": dest[i%10]} for i in range(start, start + count)]\n",
    "    return data\n",
    "\n",
    "# Creates the Dataframe\n",
    "def create_json_df(spark, data):\n",
    "    sc = spark.sparkContext\n",
    "    return spark.read.json(sc.parallelize(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulk Insert the Initial Dataset\n",
    "\n",
    "Let's generate 2M records to load into our Data Lake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "443415e8745a4883bffd2022c0cbaa45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000\n",
      "+-------------+--------+-------+-------------------+\n",
      "|  destination|route_id|trip_id|             tstamp|\n",
      "+-------------+--------+-------+-------------------+\n",
      "|      Seattle|       A|      0|2020-12-16 16:00:00|\n",
      "|     New York|       B|      1|2020-12-16 16:00:00|\n",
      "|   New Jersey|       C|      2|2020-12-16 16:00:00|\n",
      "|  Los Angeles|       D|      3|2020-12-16 16:00:00|\n",
      "|    Las Vegas|       E|      4|2020-12-16 16:00:00|\n",
      "|       Tucson|       F|      5|2020-12-16 16:00:00|\n",
      "|Washington DC|       G|      6|2020-12-16 16:00:00|\n",
      "| Philadelphia|       H|      7|2020-12-16 16:00:00|\n",
      "|        Miami|       I|      8|2020-12-16 16:00:00|\n",
      "|San Francisco|       J|      9|2020-12-16 16:00:00|\n",
      "|      Seattle|       A|     10|2020-12-16 16:00:00|\n",
      "|     New York|       B|     11|2020-12-16 16:00:00|\n",
      "|   New Jersey|       C|     12|2020-12-16 16:00:00|\n",
      "|  Los Angeles|       D|     13|2020-12-16 16:00:00|\n",
      "|    Las Vegas|       E|     14|2020-12-16 16:00:00|\n",
      "|       Tucson|       F|     15|2020-12-16 16:00:00|\n",
      "|Washington DC|       G|     16|2020-12-16 16:00:00|\n",
      "| Philadelphia|       H|     17|2020-12-16 16:00:00|\n",
      "|        Miami|       I|     18|2020-12-16 16:00:00|\n",
      "|San Francisco|       J|     19|2020-12-16 16:00:00|\n",
      "+-------------+--------+-------+-------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "dest = [\"Seattle\", \"New York\", \"New Jersey\", \"Los Angeles\", \"Las Vegas\", \"Tucson\",\"Washington DC\",\"Philadelphia\",\"Miami\",\"San Francisco\"]\n",
    "df1 = create_json_df(spark, get_json_data(0, 2000000, dest))\n",
    "print(df1.count())\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And write the data to S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d24f2a2b73f54c0eb644b575536a641f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df1.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, BULK_INSERT_OPERATION_OPT_VAL)\n",
    "      .option(BULK_INSERT_PARALLELISM, 3)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)\n",
    "      .mode(\"Overwrite\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe the number of files in S3. Expected number of files is 3 files as BULK_INSERT_PARALLELISM is set to 3. \n",
    "```\n",
    "\n",
    "$ aws s3 ls s3://<your-bucket>/demos/hudi/hudi_trips_table/ --summarize --human-readable\n",
    "                           PRE .hoodie/\n",
    "2020-04-28 23:11:39    0 Bytes .hoodie_$folder$\n",
    "2020-04-28 23:11:52   93 Bytes .hoodie_partition_metadata\n",
    "2020-04-28 23:11:59    4.8 MiB 0eeafaf2-0110-4056-b509-724b11a4c0b6-0_0-7-67_20200428231141.parquet\n",
    "2020-04-28 23:11:59    4.4 MiB 1ce9cbac-56e4-477e-a2bc-33fe62b3550f-0_1-7-68_20200428231141.parquet\n",
    "2020-04-28 23:11:59    4.6 MiB 579443dc-8f56-4990-bed6-a527f21e9682-0_2-7-69_20200428231141.parquet\n",
    "\n",
    "Total Objects: 5\n",
    "   Total Size: 13.8 MiB\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "Let's inspect the table created and query the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea6cfbbdae2f4948862bee01d0a0d13d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+-----------+\n",
      "|database|tableName       |isTemporary|\n",
      "+--------+----------------+-----------+\n",
      "|default |hudi_trips_table|false      |\n",
      "|default |test            |false      |\n",
      "+--------+----------------+-----------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e38101d63b8407088a7f71c7c812c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|createtab_stmt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|CREATE EXTERNAL TABLE `hudi_trips_table`(`_hoodie_commit_time` STRING, `_hoodie_commit_seqno` STRING, `_hoodie_record_key` STRING, `_hoodie_partition_path` STRING, `_hoodie_file_name` STRING, `destination` STRING, `route_id` STRING, `trip_id` BIGINT, `tstamp` STRING)\n",
      "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
      "WITH SERDEPROPERTIES (\n",
      "  'serialization.format' = '1'\n",
      ")\n",
      "STORED AS\n",
      "  INPUTFORMAT 'org.apache.hudi.hadoop.HoodieParquetInputFormat'\n",
      "  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\n",
      "LOCATION 's3://rr-iad-analytics-demos/emr/hudi-demo1/hudi_trips_table'\n",
      "TBLPROPERTIES (\n",
      "  'transient_lastDdlTime' = '1608131030',\n",
      "  'last_commit_time_sync' = '20201216160014'\n",
      ")\n",
      "|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show create table \"+ config['table_name']).show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the extra columns that are added by Hudi to keep track of commits and filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "152a9459ea04488c9064fb67e9e059ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+---------------------------------------------------------------------+-------------+--------+-------+-------------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|_hoodie_file_name                                                    |destination  |route_id|trip_id|tstamp             |\n",
      "+-------------------+--------------------+------------------+----------------------+---------------------------------------------------------------------+-------------+--------+-------+-------------------+\n",
      "|20201216160014     |20201216160014_0_1  |0                 |                      |38da4b30-08ae-4b9c-a51f-e3c54adf4b19-0_0-8-803_20201216160014.parquet|Seattle      |A       |0      |2020-12-16 16:00:00|\n",
      "|20201216160014     |20201216160014_0_2  |1                 |                      |38da4b30-08ae-4b9c-a51f-e3c54adf4b19-0_0-8-803_20201216160014.parquet|New York     |B       |1      |2020-12-16 16:00:00|\n",
      "|20201216160014     |20201216160014_0_3  |10                |                      |38da4b30-08ae-4b9c-a51f-e3c54adf4b19-0_0-8-803_20201216160014.parquet|Seattle      |A       |10     |2020-12-16 16:00:00|\n",
      "|20201216160014     |20201216160014_0_4  |100               |                      |38da4b30-08ae-4b9c-a51f-e3c54adf4b19-0_0-8-803_20201216160014.parquet|Seattle      |A       |100    |2020-12-16 16:00:00|\n",
      "|20201216160014     |20201216160014_0_5  |1000              |                      |38da4b30-08ae-4b9c-a51f-e3c54adf4b19-0_0-8-803_20201216160014.parquet|Seattle      |A       |1000   |2020-12-16 16:00:00|\n",
      "|20201216160014     |20201216160014_0_6  |10000             |                      |38da4b30-08ae-4b9c-a51f-e3c54adf4b19-0_0-8-803_20201216160014.parquet|Seattle      |A       |10000  |2020-12-16 16:00:00|\n",
      "|20201216160014     |20201216160014_0_7  |100000            |                      |38da4b30-08ae-4b9c-a51f-e3c54adf4b19-0_0-8-803_20201216160014.parquet|Seattle      |A       |100000 |2020-12-16 16:00:00|\n",
      "|20201216160014     |20201216160014_0_8  |1000000           |                      |38da4b30-08ae-4b9c-a51f-e3c54adf4b19-0_0-8-803_20201216160014.parquet|Seattle      |A       |1000000|2020-12-16 16:00:00|\n",
      "|20201216160014     |20201216160014_0_9  |1000001           |                      |38da4b30-08ae-4b9c-a51f-e3c54adf4b19-0_0-8-803_20201216160014.parquet|New York     |B       |1000001|2020-12-16 16:00:00|\n",
      "|20201216160014     |20201216160014_0_10 |1000002           |                      |38da4b30-08ae-4b9c-a51f-e3c54adf4b19-0_0-8-803_20201216160014.parquet|New Jersey   |C       |1000002|2020-12-16 16:00:00|\n",
      "|20201216160014     |20201216160014_0_11 |1000003           |                      |38da4b30-08ae-4b9c-a51f-e3c54adf4b19-0_0-8-803_20201216160014.parquet|Los Angeles  |D       |1000003|2020-12-16 16:00:00|\n",
      "|20201216160014     |20201216160014_0_12 |1000004           |                      |38da4b30-08ae-4b9c-a51f-e3c54adf4b19-0_0-8-803_20201216160014.parquet|Las Vegas    |E       |1000004|2020-12-16 16:00:00|\n",
      "|20201216160014     |20201216160014_0_13 |1000005           |                      |38da4b30-08ae-4b9c-a51f-e3c54adf4b19-0_0-8-803_20201216160014.parquet|Tucson       |F       |1000005|2020-12-16 16:00:00|\n",
      "|20201216160014     |20201216160014_0_14 |1000006           |                      |38da4b30-08ae-4b9c-a51f-e3c54adf4b19-0_0-8-803_20201216160014.parquet|Washington DC|G       |1000006|2020-12-16 16:00:00|\n",
      "|20201216160014     |20201216160014_0_15 |1000007           |                      |38da4b30-08ae-4b9c-a51f-e3c54adf4b19-0_0-8-803_20201216160014.parquet|Philadelphia |H       |1000007|2020-12-16 16:00:00|\n",
      "|20201216160014     |20201216160014_0_16 |1000008           |                      |38da4b30-08ae-4b9c-a51f-e3c54adf4b19-0_0-8-803_20201216160014.parquet|Miami        |I       |1000008|2020-12-16 16:00:00|\n",
      "|20201216160014     |20201216160014_0_17 |1000009           |                      |38da4b30-08ae-4b9c-a51f-e3c54adf4b19-0_0-8-803_20201216160014.parquet|San Francisco|J       |1000009|2020-12-16 16:00:00|\n",
      "|20201216160014     |20201216160014_0_18 |100001            |                      |38da4b30-08ae-4b9c-a51f-e3c54adf4b19-0_0-8-803_20201216160014.parquet|New York     |B       |100001 |2020-12-16 16:00:00|\n",
      "|20201216160014     |20201216160014_0_19 |1000010           |                      |38da4b30-08ae-4b9c-a51f-e3c54adf4b19-0_0-8-803_20201216160014.parquet|Seattle      |A       |1000010|2020-12-16 16:00:00|\n",
      "|20201216160014     |20201216160014_0_20 |1000011           |                      |38da4b30-08ae-4b9c-a51f-e3c54adf4b19-0_0-8-803_20201216160014.parquet|New York     |B       |1000011|2020-12-16 16:00:00|\n",
      "+-------------------+--------------------+------------------+----------------------+---------------------------------------------------------------------+-------------+--------+-------+-------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "df2=spark.read.format(HUDI_FORMAT).load(config[\"target\"]+\"/*\")\n",
    "df2.count()\n",
    "df2.show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can query the Hive table as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51dbb3901d3a49aeb7fea179fe46b0e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|2000000 |\n",
      "+--------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from \"+ config['table_name']).show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Upsert some records\n",
    "\n",
    "Let's modify a few records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63fe26741dec4280b64b9719e905f258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------+-------------------+\n",
      "|trip_id|route_id|destination  |tstamp             |\n",
      "+-------+--------+-------------+-------------------+\n",
      "|1000000|A       |Seattle      |2020-12-16 16:00:00|\n",
      "|1000001|B       |New York     |2020-12-16 16:00:00|\n",
      "|1000002|C       |New Jersey   |2020-12-16 16:00:00|\n",
      "|1000003|D       |Los Angeles  |2020-12-16 16:00:00|\n",
      "|1000004|E       |Las Vegas    |2020-12-16 16:00:00|\n",
      "|1000005|F       |Tucson       |2020-12-16 16:00:00|\n",
      "|1000006|G       |Washington DC|2020-12-16 16:00:00|\n",
      "|1000007|H       |Philadelphia |2020-12-16 16:00:00|\n",
      "|1000008|I       |Miami        |2020-12-16 16:00:00|\n",
      "|1000009|J       |San Francisco|2020-12-16 16:00:00|\n",
      "+-------+--------+-------------+-------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trip_id, route_id, destination, tstamp from \"+config['table_name'] +\" where trip_id between 1000000 and 1000009\").show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52f9576d0d1547a1871e128fcd1f50ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10"
     ]
    }
   ],
   "source": [
    "upsert_dest = [\"Boston\", \"Boston\", \"Boston\", \"Boston\", \"Boston\",\"Boston\",\"Boston\",\"Boston\",\"Boston\",\"Boston\"]\n",
    "df3 = create_json_df(spark, get_json_data(1000000, 10, upsert_dest))\n",
    "df3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "975cbcfb55c84dbdb54760a493a98555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------------+-----------+\n",
      "|trip_id|route_id|             tstamp|destination|\n",
      "+-------+--------+-------------------+-----------+\n",
      "|1000000|       A|2020-12-16 16:02:59|     Boston|\n",
      "|1000001|       B|2020-12-16 16:02:59|     Boston|\n",
      "|1000002|       C|2020-12-16 16:02:59|     Boston|\n",
      "|1000003|       D|2020-12-16 16:02:59|     Boston|\n",
      "|1000004|       E|2020-12-16 16:02:59|     Boston|\n",
      "|1000005|       F|2020-12-16 16:02:59|     Boston|\n",
      "|1000006|       G|2020-12-16 16:02:59|     Boston|\n",
      "|1000007|       H|2020-12-16 16:02:59|     Boston|\n",
      "|1000008|       I|2020-12-16 16:02:59|     Boston|\n",
      "|1000009|       J|2020-12-16 16:02:59|     Boston|\n",
      "+-------+--------+-------------------+-----------+"
     ]
    }
   ],
   "source": [
    "df3.select(\"trip_id\",\"route_id\",\"tstamp\",\"destination\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have changed the destination and timestamp for trip IDs 1000000 to 1000010. Now, let's upsert the changes to S3. Note that the operation now is \"Upsert\" as opposed to BulkInsert for the initial load:\n",
    "\n",
    "```\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8599e279eeed46d08f8cf9c470508fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df3.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "      .option(UPSERT_PARALLELISM, 20)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HUDI_CLEANER_POLICY, KEEP_LATEST_COMMITS)\n",
    "      .option(HUDI_COMMITS_RETAINED,config[\"commits_to_retain\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)  \n",
    "      .mode(\"Append\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac835b0e04814a45b2602c85fcc01987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------------+-------------+\n",
      "|trip_id|route_id|tstamp             |destination  |\n",
      "+-------+--------+-------------------+-------------+\n",
      "|1000000|A       |2020-12-16 16:02:59|Boston       |\n",
      "|1000001|B       |2020-12-16 16:02:59|Boston       |\n",
      "|1000002|C       |2020-12-16 16:02:59|Boston       |\n",
      "|1000003|D       |2020-12-16 16:02:59|Boston       |\n",
      "|1000004|E       |2020-12-16 16:02:59|Boston       |\n",
      "|1000005|F       |2020-12-16 16:02:59|Boston       |\n",
      "|1000006|G       |2020-12-16 16:02:59|Boston       |\n",
      "|1000007|H       |2020-12-16 16:02:59|Boston       |\n",
      "|1000008|I       |2020-12-16 16:02:59|Boston       |\n",
      "|1000009|J       |2020-12-16 16:02:59|Boston       |\n",
      "|1000010|A       |2020-12-16 16:00:00|Seattle      |\n",
      "|1000011|B       |2020-12-16 16:00:00|New York     |\n",
      "|1000012|C       |2020-12-16 16:00:00|New Jersey   |\n",
      "|1000013|D       |2020-12-16 16:00:00|Los Angeles  |\n",
      "|999996 |G       |2020-12-16 16:00:00|Washington DC|\n",
      "|999997 |H       |2020-12-16 16:00:00|Philadelphia |\n",
      "|999998 |I       |2020-12-16 16:00:00|Miami        |\n",
      "|999999 |J       |2020-12-16 16:00:00|San Francisco|\n",
      "+-------+--------+-------------------+-------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trip_id, route_id, tstamp, destination from \"+config['table_name'] +\" where trip_id between 999996 and 1000013\").show(50,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check the commit\n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://<your-bucket>/demos/hudi/hudi_trips_table/ --summarize --human-readable\n",
    "                           PRE .hoodie/\n",
    "2020-04-28 23:11:39    0 Bytes .hoodie_$folder$\n",
    "2020-04-28 23:11:52   93 Bytes .hoodie_partition_metadata\n",
    "2020-04-28 23:15:43    4.8 MiB 0eeafaf2-0110-4056-b509-724b11a4c0b6-0_0-51-609_20200428231528.parquet\n",
    "2020-04-28 23:11:59    4.8 MiB 0eeafaf2-0110-4056-b509-724b11a4c0b6-0_0-7-67_20200428231141.parquet\n",
    "2020-04-28 23:11:59    4.4 MiB 1ce9cbac-56e4-477e-a2bc-33fe62b3550f-0_1-7-68_20200428231141.parquet\n",
    "2020-04-28 23:11:59    4.6 MiB 579443dc-8f56-4990-bed6-a527f21e9682-0_2-7-69_20200428231141.parquet\n",
    "\n",
    "Total Objects: 6\n",
    "   Total Size: 18.6 MiB\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we upserted some records, let us try to insert 10 new records into the table. We will use same upsert option. As primary keys 2000000 to 2000009 do not exist in the table, the records will be inserted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cd66ffef0bc413b866e698c1f56dc7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------+-------------------+\n",
      "|destination|route_id|trip_id|             tstamp|\n",
      "+-----------+--------+-------+-------------------+\n",
      "|   Syracuse|       A|2000000|2020-12-16 16:05:38|\n",
      "|   Syracuse|       B|2000001|2020-12-16 16:05:38|\n",
      "|   Syracuse|       C|2000002|2020-12-16 16:05:38|\n",
      "|   Syracuse|       D|2000003|2020-12-16 16:05:38|\n",
      "|   Syracuse|       E|2000004|2020-12-16 16:05:38|\n",
      "|   Syracuse|       F|2000005|2020-12-16 16:05:38|\n",
      "|   Syracuse|       G|2000006|2020-12-16 16:05:38|\n",
      "|   Syracuse|       H|2000007|2020-12-16 16:05:38|\n",
      "|   Syracuse|       I|2000008|2020-12-16 16:05:38|\n",
      "|   Syracuse|       J|2000009|2020-12-16 16:05:38|\n",
      "+-----------+--------+-------+-------------------+"
     ]
    }
   ],
   "source": [
    "insert_dest = [\"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\"]\n",
    "df5 = create_json_df(spark, get_json_data(2000000, 10, insert_dest))\n",
    "df5.count()\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e58ad8f13e0f469eb234a09fe1dad28d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df5.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "      .option(UPSERT_PARALLELISM, 20)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HUDI_CLEANER_POLICY, KEEP_LATEST_COMMITS)\n",
    "      .option(HUDI_COMMITS_RETAINED,config[\"commits_to_retain\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)  \n",
    "      .mode(\"Append\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "461f3c3cd96b4ccab241d0e51d59c058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000010"
     ]
    }
   ],
   "source": [
    "df6=spark.read.format(HUDI_FORMAT).load(config[\"target\"]+\"/*\")\n",
    "df6.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06a6089b55fb4a898fc3ff6572150979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------------+-------------+\n",
      "|trip_id|route_id|tstamp             |destination  |\n",
      "+-------+--------+-------------------+-------------+\n",
      "|2000009|J       |2020-12-16 16:05:38|Syracuse     |\n",
      "|2000008|I       |2020-12-16 16:05:38|Syracuse     |\n",
      "|2000007|H       |2020-12-16 16:05:38|Syracuse     |\n",
      "|2000006|G       |2020-12-16 16:05:38|Syracuse     |\n",
      "|2000001|B       |2020-12-16 16:05:38|Syracuse     |\n",
      "|2000000|A       |2020-12-16 16:05:38|Syracuse     |\n",
      "|2000005|F       |2020-12-16 16:05:38|Syracuse     |\n",
      "|2000004|E       |2020-12-16 16:05:38|Syracuse     |\n",
      "|2000003|D       |2020-12-16 16:05:38|Syracuse     |\n",
      "|2000002|C       |2020-12-16 16:05:38|Syracuse     |\n",
      "|1999997|H       |2020-12-16 16:00:00|Philadelphia |\n",
      "|1999998|I       |2020-12-16 16:00:00|Miami        |\n",
      "|1999999|J       |2020-12-16 16:00:00|San Francisco|\n",
      "+-------+--------+-------------------+-------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trip_id, route_id, tstamp, destination from \"+config['table_name'] +\" where trip_id > 1999996\").show(50,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the records are updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting Records.\n",
    "\n",
    "Apache Hudi supports implementing two types of deletes on data stored in Hudi datasets, by enabling the user to specify a different record payload implementation.\n",
    "\n",
    "* **Soft Deletes** : With soft deletes, user wants to retain the key but just null out the values for all other fields. This can be simply achieved by ensuring the appropriate fields are nullable in the dataset schema and simply upserting the dataset after setting these fields to null.\n",
    "    \n",
    "* **Hard Deletes** : A stronger form of delete is to physically remove any trace of the record from the dataset. \n",
    "\n",
    "Let's now execute some hard delete operations on our dataset which will remove the records from our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's delete the 10 records with the \"Syracuse\" destination we added to the table. Note that the only change is the single line that set the hoodie.datasource.write.payload.class to org.apache.hudi.EmptyHoodieRecordPayload to delete the records.\n",
    "\n",
    "```\n",
    ".option(PAYLOAD_CLASS_OPT_KEY, EMPTY_PAYLOAD_CLASS_OPT_VAL)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27b5355615dd48ea9bfafe7238ecc598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------+-------------------+\n",
      "|destination|route_id|trip_id|             tstamp|\n",
      "+-----------+--------+-------+-------------------+\n",
      "|   Syracuse|       A|2000000|2020-12-16 16:05:38|\n",
      "|   Syracuse|       B|2000001|2020-12-16 16:05:38|\n",
      "|   Syracuse|       C|2000002|2020-12-16 16:05:38|\n",
      "|   Syracuse|       D|2000003|2020-12-16 16:05:38|\n",
      "|   Syracuse|       E|2000004|2020-12-16 16:05:38|\n",
      "|   Syracuse|       F|2000005|2020-12-16 16:05:38|\n",
      "|   Syracuse|       G|2000006|2020-12-16 16:05:38|\n",
      "|   Syracuse|       H|2000007|2020-12-16 16:05:38|\n",
      "|   Syracuse|       I|2000008|2020-12-16 16:05:38|\n",
      "|   Syracuse|       J|2000009|2020-12-16 16:05:38|\n",
      "+-----------+--------+-------+-------------------+"
     ]
    }
   ],
   "source": [
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d761e65900a54c9d9d0fa74d4d1c8a19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df5.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "      .option(UPSERT_PARALLELISM, 20)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HUDI_CLEANER_POLICY, KEEP_LATEST_COMMITS)\n",
    "      .option(HUDI_COMMITS_RETAINED,config[\"commits_to_retain\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)\n",
    "      .option(PAYLOAD_CLASS_OPT_KEY, EMPTY_PAYLOAD_CLASS_OPT_VAL)\n",
    "      .mode(\"Append\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de4fc31cdd9489fb7cc0072dec8934b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------------+-------------+\n",
      "|trip_id|route_id|tstamp             |destination  |\n",
      "+-------+--------+-------------------+-------------+\n",
      "|1999997|H       |2020-12-16 16:00:00|Philadelphia |\n",
      "|1999998|I       |2020-12-16 16:00:00|Miami        |\n",
      "|1999999|J       |2020-12-16 16:00:00|San Francisco|\n",
      "+-------+--------+-------------------+-------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trip_id, route_id, tstamp, destination from \"+config['table_name'] +\" where trip_id > 1999996\").show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the records > 2000000 no longer exist in our table.\n",
    "\n",
    "Let's observe the number of files in S3. Expected : 6 files (initial files (3) + one upsert + one insert + one delete = 6)\n",
    "\n",
    "```\n",
    "\n",
    "$ aws s3 ls s3://<your-bucket>/demos/hudi/hudi_trips_table/ --summarize --human-readable\n",
    "                           PRE .hoodie/\n",
    "2020-04-28 23:11:39    0 Bytes .hoodie_$folder$\n",
    "2020-04-28 23:11:52   93 Bytes .hoodie_partition_metadata\n",
    "2020-04-28 23:15:43    4.8 MiB 0eeafaf2-0110-4056-b509-724b11a4c0b6-0_0-51-609_20200428231528.parquet\n",
    "2020-04-28 23:11:59    4.8 MiB 0eeafaf2-0110-4056-b509-724b11a4c0b6-0_0-7-67_20200428231141.parquet\n",
    "2020-04-28 23:18:24    4.4 MiB 1ce9cbac-56e4-477e-a2bc-33fe62b3550f-0_0-135-1376_20200428231814.parquet\n",
    "2020-04-28 23:17:17    4.4 MiB 1ce9cbac-56e4-477e-a2bc-33fe62b3550f-0_0-93-1007_20200428231705.parquet\n",
    "2020-04-28 23:11:59    4.4 MiB 1ce9cbac-56e4-477e-a2bc-33fe62b3550f-0_1-7-68_20200428231141.parquet\n",
    "2020-04-28 23:11:59    4.6 MiB 579443dc-8f56-4990-bed6-a527f21e9682-0_2-7-69_20200428231141.parquet\n",
    "\n",
    "Total Objects: 8\n",
    "   Total Size: 27.5 MiB\n",
    "\n",
    "```\n",
    "\n",
    "In our example, we set number of commits to retain as 10. So, maximum only 10 new files can be created on top of our bulk insert files. i.e., 13 files in total. If we had set the commits_to_retain as 2, the number of files created will not increase beyond initial files(3) + commits_to_retain(2) = 5 files. This is because Hudi Cleaning Policy will delete older files when writing based on the commit retain policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rollback\n",
    "\n",
    "Let's say we want to roll back the last delete we made. \n",
    "\n",
    "/usr/lib/hudi/cli/bin/hudi-cli.sh\n",
    "\n",
    "connect --path s3://<your-bucket>/demos/hudi/hudi_trips_table/\n",
    "\n",
    "commits show\n",
    "\n",
    "hudi:hudi_trips_table->commits show\n",
    "20/04/28 23:46:09 INFO s3n.S3NativeFileSystem: Opening 's3://<Your S3 Bucket Here>/tmp/hudi/hudi_trips_table/.hoodie/20200428231705.commit' for reading\n",
    "20/04/28 23:46:09 INFO s3n.S3NativeFileSystem: Opening 's3://<Your S3 Bucket Here>/tmp/hudi/hudi_trips_table/.hoodie/20200428231528.commit' for reading\n",
    "20/04/28 23:46:09 INFO s3n.S3NativeFileSystem: Opening 's3://<Your S3 Bucket Here>/tmp/hudi/hudi_trips_table/.hoodie/20200428231141.commit' for reading\n",
    "╔════════════════╤═════════════════════╤═══════════════════╤═════════════════════╤══════════════════════════╤═══════════════════════╤══════════════════════════════╤══════════════╗\n",
    "║ CommitTime     │ Total Bytes Written │ Total Files Added │ Total Files Updated │ Total Partitions Written │ Total Records Written │ Total Update Records Written │ Total Errors ║\n",
    "╠════════════════╪═════════════════════╪═══════════════════╪═════════════════════╪══════════════════════════╪═══════════════════════╪══════════════════════════════╪══════════════╣\n",
    "║ 20200428231814 │ 4.4 MB              │ 0                 │ 1                   │ 1                        │ 642132                │ 0                            │ 0            ║\n",
    "╟────────────────┼─────────────────────┼───────────────────┼─────────────────────┼──────────────────────────┼───────────────────────┼──────────────────────────────┼──────────────╢\n",
    "║ 20200428231528 │ 4.8 MB              │ 0                 │ 1                   │ 1                        │ 697329                │ 10                           │ 0            ║\n",
    "╟────────────────┼─────────────────────┼───────────────────┼─────────────────────┼──────────────────────────┼───────────────────────┼──────────────────────────────┼──────────────╢\n",
    "║ 20200428231528 │ 4.8 MB              │ 0                 │ 1                   │ 1                        │ 697329                │ 10                           │ 0            ║\n",
    "╟────────────────┼─────────────────────┼───────────────────┼─────────────────────┼──────────────────────────┼───────────────────────┼──────────────────────────────┼──────────────╢\n",
    "\n",
    "║ 20200428231141 │ 13.8 MB             │ 3                 │ 0                   │ 1                        │ 2000000               │ 0                            │ 0            ║\n",
    "╚════════════════╧═════════════════════╧═══════════════════╧═════════════════════╧══════════════════════════╧═══════════════════════╧══════════════════════════════╧══════════════╝\n",
    "\n",
    "commit rollback --commit 20200428231814\n",
    "    \n",
    "20/12/16 16:13:13 INFO YarnClientSchedulerBackend: Stopped\n",
    "20/12/16 16:13:13 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
    "20/12/16 16:13:13 INFO MemoryStore: MemoryStore cleared\n",
    "20/12/16 16:13:13 INFO BlockManager: BlockManager stopped\n",
    "20/12/16 16:13:13 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
    "20/12/16 16:13:13 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
    "20/12/16 16:13:13 INFO SparkContext: Successfully stopped SparkContext\n",
    "20/12/16 16:13:13 INFO ShutdownHookManager: Shutdown hook called\n",
    "20/12/16 16:13:13 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-83dbf860-4688-4cb8-a0a3-c8c4f2b9ab5f\n",
    "20/12/16 16:13:13 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-7ce7bb16-b4a4-4ce0-b97b-0343c7c7568d\n",
    "hudi:hudi_trips_table->20/12/16 16:13:13 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from s3://rr-iad-analytics-demos/emr/hudi-demo1/hudi_trips_table/\n",
    "20/12/16 16:13:13 INFO fs.FSUtils: Hadoop Configuration: fs.defaultFS: [hdfs://ip-10-31-2-65.ec2.internal:8020], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, emrfs-site.xml], FileSystem: [com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem@192fff47]\n",
    "20/12/16 16:13:13 INFO table.HoodieTableConfig: Loading table properties from s3://rr-iad-analytics-demos/emr/hudi-demo1/hudi_trips_table/.hoodie/hoodie.properties\n",
    "20/12/16 16:13:13 INFO s3n.S3NativeFileSystem: Opening 's3://<your-bucket>/demos/hudi/hudi_trips_table/.hoodie/hoodie.properties' for reading\n",
    "20/12/16 16:13:13 INFO table.HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3://<your-bucket>/demos/hudi//hudi-demo1/hudi_trips_table/\n",
    "Commit 20201216160714 rolled back\n",
    "hudi:hudi_trips_table->\n",
    "    \n",
    "Now let us check what happened to the records we deleted earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c73ffe696ad44884aaa32b5aa63e764d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------------+-------------+\n",
      "|trip_id|route_id|tstamp             |destination  |\n",
      "+-------+--------+-------------------+-------------+\n",
      "|2000009|J       |2020-12-16 16:05:38|Syracuse     |\n",
      "|2000008|I       |2020-12-16 16:05:38|Syracuse     |\n",
      "|2000007|H       |2020-12-16 16:05:38|Syracuse     |\n",
      "|2000006|G       |2020-12-16 16:05:38|Syracuse     |\n",
      "|2000001|B       |2020-12-16 16:05:38|Syracuse     |\n",
      "|2000000|A       |2020-12-16 16:05:38|Syracuse     |\n",
      "|2000005|F       |2020-12-16 16:05:38|Syracuse     |\n",
      "|2000004|E       |2020-12-16 16:05:38|Syracuse     |\n",
      "|2000003|D       |2020-12-16 16:05:38|Syracuse     |\n",
      "|2000002|C       |2020-12-16 16:05:38|Syracuse     |\n",
      "|1999997|H       |2020-12-16 16:00:00|Philadelphia |\n",
      "|1999998|I       |2020-12-16 16:00:00|Miami        |\n",
      "|1999999|J       |2020-12-16 16:00:00|San Francisco|\n",
      "+-------+--------+-------------------+-------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trip_id, route_id, tstamp, destination from \"+config['table_name'] +\" where trip_id > 1999996\").show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Travel with Hudi \n",
    "\n",
    "Now, let us time travel with Hudi (query previous commits) with incremental and point-in-time queries\n",
    "\n",
    "First, lets check out incremental queries\n",
    "\n",
    "In EMR cluster, let us check Hudi CLI and check how commits look right now \n",
    "\n",
    "/usr/lib/hudi/cli/bin/hudi-cli.sh\n",
    "\n",
    "connect --path s3://<your-bucket>/demos/hudi/hudi_trips_table/\n",
    "\n",
    "commits show\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db927a9391c9434394aa79b7cf3ec6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|commitTime    |\n",
      "+--------------+\n",
      "|20201216160014|\n",
      "|20201216160326|\n",
      "|20201216160607|\n",
      "+--------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select distinct(_hoodie_commit_time) as commitTime from hudi_trips_table order by commitTime\").show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b7e8c16dac4af69a4967a4b61b958a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commits: \n",
      "Row(commitTime='20201216160014')\n",
      "Row(commitTime='20201216160326')\n",
      "Row(commitTime='20201216160607')\n",
      "begin time: 20201216160014\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----------+--------+-------+-------------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|destination|route_id|trip_id|             tstamp|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----------+--------+-------+-------------------+\n",
      "|     20201216160326|20201216160326_0_...|           1000000|                      |38da4b30-08ae-4b9...|     Boston|       A|1000000|2020-12-16 16:02:59|\n",
      "|     20201216160326|20201216160326_0_...|           1000001|                      |38da4b30-08ae-4b9...|     Boston|       B|1000001|2020-12-16 16:02:59|\n",
      "|     20201216160326|20201216160326_0_...|           1000002|                      |38da4b30-08ae-4b9...|     Boston|       C|1000002|2020-12-16 16:02:59|\n",
      "|     20201216160326|20201216160326_0_...|           1000003|                      |38da4b30-08ae-4b9...|     Boston|       D|1000003|2020-12-16 16:02:59|\n",
      "|     20201216160326|20201216160326_0_...|           1000004|                      |38da4b30-08ae-4b9...|     Boston|       E|1000004|2020-12-16 16:02:59|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----------+--------+-------+-------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "commits = spark.sql(\"select distinct(_hoodie_commit_time) as commitTime from  hudi_trips_table order by commitTime\").collect()\n",
    "print(\"commits: \")\n",
    "for elem in commits: print (elem) \n",
    "\n",
    "beginTime = commits[-3][0] # commit time we are interested in\n",
    "print(\"begin time: \" + beginTime)\n",
    "\n",
    "# incrementally query data\n",
    "incViewDF = spark.read.format(\"org.apache.hudi\") \\\n",
    "                   .option(VIEW_TYPE_OPT_KEY, VIEW_TYPE_INCREMENTAL_OPT_VAL) \\\n",
    "                   .option(BEGIN_INSTANTTIME_OPT_KEY, beginTime) \\\n",
    "                   .load(config[\"target\"]) \n",
    "\n",
    "incViewDF.show(5)\n",
    "incViewDF.registerTempTable(\"hudi_incr_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bff57b967d44d17977f2e02ee40c163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+--------+-----------+-------------------+\n",
      "|_hoodie_commit_time|trip_id|route_id|destination|             tstamp|\n",
      "+-------------------+-------+--------+-----------+-------------------+\n",
      "|     20201216160326|1000000|       A|     Boston|2020-12-16 16:02:59|\n",
      "|     20201216160326|1000001|       B|     Boston|2020-12-16 16:02:59|\n",
      "|     20201216160326|1000002|       C|     Boston|2020-12-16 16:02:59|\n",
      "|     20201216160326|1000003|       D|     Boston|2020-12-16 16:02:59|\n",
      "|     20201216160326|1000004|       E|     Boston|2020-12-16 16:02:59|\n",
      "|     20201216160326|1000005|       F|     Boston|2020-12-16 16:02:59|\n",
      "|     20201216160326|1000006|       G|     Boston|2020-12-16 16:02:59|\n",
      "|     20201216160326|1000007|       H|     Boston|2020-12-16 16:02:59|\n",
      "|     20201216160326|1000008|       I|     Boston|2020-12-16 16:02:59|\n",
      "|     20201216160326|1000009|       J|     Boston|2020-12-16 16:02:59|\n",
      "+-------------------+-------+--------+-----------+-------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select `_hoodie_commit_time`, trip_id, route_id, destination, tstamp from  hudi_incr_table where trip_id between 999996 and 1000013\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets check out point-in-time queries. i.e., query from a specific commit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c46771509b5f47e9bc25063f3e431fe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commits: \n",
      "Row(commitTime='20201216160014')\n",
      "Row(commitTime='20201216160326')\n",
      "Row(commitTime='20201216160607')"
     ]
    }
   ],
   "source": [
    "commits = spark.sql(\"select distinct(_hoodie_commit_time) as commitTime from  hudi_trips_table order by commitTime\").collect()\n",
    "print(\"commits: \")\n",
    "for elem in commits: print (elem) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e5d16730494ba8983f805edbafa525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end time: 20201216160014\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----------+--------+-------+-------------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|destination|route_id|trip_id|             tstamp|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----------+--------+-------+-------------------+\n",
      "|     20201216160014|  20201216160014_0_1|                 0|                      |38da4b30-08ae-4b9...|    Seattle|       A|      0|2020-12-16 16:00:00|\n",
      "|     20201216160014|  20201216160014_0_2|                 1|                      |38da4b30-08ae-4b9...|   New York|       B|      1|2020-12-16 16:00:00|\n",
      "|     20201216160014|  20201216160014_0_3|                10|                      |38da4b30-08ae-4b9...|    Seattle|       A|     10|2020-12-16 16:00:00|\n",
      "|     20201216160014|  20201216160014_0_4|               100|                      |38da4b30-08ae-4b9...|    Seattle|       A|    100|2020-12-16 16:00:00|\n",
      "|     20201216160014|  20201216160014_0_5|              1000|                      |38da4b30-08ae-4b9...|    Seattle|       A|   1000|2020-12-16 16:00:00|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----------+--------+-------+-------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "#Lets check out the first state. i.e., before we updated the trip to Boston\n",
    "\n",
    "beginTime = \"000\" #Represents all commits > this time.\n",
    "endTime = commits[-3][0] # first commit\n",
    "print(\"end time: \"+endTime)\n",
    "\n",
    "# incrementally query data\n",
    "incViewDF = spark.read.format(\"org.apache.hudi\") \\\n",
    "                   .option(VIEW_TYPE_OPT_KEY, VIEW_TYPE_INCREMENTAL_OPT_VAL) \\\n",
    "                   .option(BEGIN_INSTANTTIME_OPT_KEY, beginTime) \\\n",
    "                   .option(END_INSTANTTIME_OPT_KEY, endTime) \\\n",
    "                   .load(config[\"target\"]) \n",
    "\n",
    "incViewDF.show(5)\n",
    "incViewDF.registerTempTable(\"hudi_incr_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52875e13084a4282b75a41bfa31ebde7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+--------+-------------+-------------------+\n",
      "|_hoodie_commit_time|trip_id|route_id|  destination|             tstamp|\n",
      "+-------------------+-------+--------+-------------+-------------------+\n",
      "|     20201216160014|1000000|       A|      Seattle|2020-12-16 16:00:00|\n",
      "|     20201216160014|1000001|       B|     New York|2020-12-16 16:00:00|\n",
      "|     20201216160014|1000002|       C|   New Jersey|2020-12-16 16:00:00|\n",
      "|     20201216160014|1000003|       D|  Los Angeles|2020-12-16 16:00:00|\n",
      "|     20201216160014|1000004|       E|    Las Vegas|2020-12-16 16:00:00|\n",
      "|     20201216160014|1000005|       F|       Tucson|2020-12-16 16:00:00|\n",
      "|     20201216160014|1000006|       G|Washington DC|2020-12-16 16:00:00|\n",
      "|     20201216160014|1000007|       H| Philadelphia|2020-12-16 16:00:00|\n",
      "|     20201216160014|1000008|       I|        Miami|2020-12-16 16:00:00|\n",
      "|     20201216160014|1000009|       J|San Francisco|2020-12-16 16:00:00|\n",
      "|     20201216160014|1000010|       A|      Seattle|2020-12-16 16:00:00|\n",
      "|     20201216160014|1000011|       B|     New York|2020-12-16 16:00:00|\n",
      "|     20201216160014|1000012|       C|   New Jersey|2020-12-16 16:00:00|\n",
      "|     20201216160014|1000013|       D|  Los Angeles|2020-12-16 16:00:00|\n",
      "|     20201216160014| 999996|       G|Washington DC|2020-12-16 16:00:00|\n",
      "|     20201216160014| 999997|       H| Philadelphia|2020-12-16 16:00:00|\n",
      "|     20201216160014| 999998|       I|        Miami|2020-12-16 16:00:00|\n",
      "|     20201216160014| 999999|       J|San Francisco|2020-12-16 16:00:00|\n",
      "+-------------------+-------+--------+-------------+-------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select `_hoodie_commit_time`, trip_id, route_id, destination, tstamp from  hudi_incr_table where trip_id between 999996 and 1000013\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that the records from 1000000 to 1000009 display first state of our table before we upserted the trip destination to Boston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge On Read \n",
    "\n",
    "The default table type is Copy-On-Write which is best suited for read-heavy workloads with modest writes. Copy-On-Write creates commit files with original data + the new changes during writing itself. While this increases latency on writes, this set up makes it more manageable for faster read.\n",
    "\n",
    "For near real-time applications that mandate quick upserts, MERGE_ON_READ table type would be better suited. MOR table stores incoming upserts for each file group, onto a row based delta log (In Avro file format). This log is then merged with the existing Parquet file using a a compactor during reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d403e7313bf41d795d64f7cb6fb3e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## CHANGE ME ##\n",
    "config = {\n",
    "    \"table_name\": \"hudi_mor_trips_table\",\n",
    "    \"table_name_rt\": \"hudi_mor_trips_table_rt\",\n",
    "    \"target\": \"s3://rr-iad-analytics-demos/emr/hudi-demo1/hudi_mor_trips_table\",\n",
    "    \"primary_key\": \"trip_id\",\n",
    "    \"sort_key\": \"tstamp\",\n",
    "    \"commits_to_retain\": \"2\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fdd71cdfe2d4bc3a2ee58915e2818fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "STORAGE_TYPE_OPT_KEY=\"hoodie.datasource.write.storage.type\"\n",
    "COMPACTION_INLINE_OPT_KEY=\"hoodie.compact.inline\"\n",
    "COMPACTION_MAX_DELTA_COMMITS_OPT_KEY=\"hoodie.compact.inline.max.delta.commits\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4df96dfbe7d24d1fb2cfb808052dd214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+-------+-------------------+\n",
      "|  destination|route_id|trip_id|             tstamp|\n",
      "+-------------+--------+-------+-------------------+\n",
      "|      Seattle|       A|      0|2020-12-16 16:20:31|\n",
      "|     New York|       B|      1|2020-12-16 16:20:31|\n",
      "|   New Jersey|       C|      2|2020-12-16 16:20:31|\n",
      "|  Los Angeles|       D|      3|2020-12-16 16:20:31|\n",
      "|    Las Vegas|       E|      4|2020-12-16 16:20:31|\n",
      "|       Tucson|       F|      5|2020-12-16 16:20:31|\n",
      "|Washington DC|       G|      6|2020-12-16 16:20:31|\n",
      "| Philadelphia|       H|      7|2020-12-16 16:20:31|\n",
      "|        Miami|       I|      8|2020-12-16 16:20:31|\n",
      "|San Francisco|       J|      9|2020-12-16 16:20:31|\n",
      "|      Seattle|       A|     10|2020-12-16 16:20:31|\n",
      "|     New York|       B|     11|2020-12-16 16:20:31|\n",
      "|   New Jersey|       C|     12|2020-12-16 16:20:31|\n",
      "|  Los Angeles|       D|     13|2020-12-16 16:20:31|\n",
      "|    Las Vegas|       E|     14|2020-12-16 16:20:31|\n",
      "|       Tucson|       F|     15|2020-12-16 16:20:31|\n",
      "|Washington DC|       G|     16|2020-12-16 16:20:31|\n",
      "| Philadelphia|       H|     17|2020-12-16 16:20:31|\n",
      "|        Miami|       I|     18|2020-12-16 16:20:31|\n",
      "|San Francisco|       J|     19|2020-12-16 16:20:31|\n",
      "+-------------+--------+-------+-------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "mor_dest = [\"Seattle\", \"New York\", \"New Jersey\", \"Los Angeles\", \"Las Vegas\", \"Tucson\",\"Washington DC\",\"Philadelphia\",\"Miami\",\"San Francisco\"]\n",
    "df2 = create_json_df(spark, get_json_data(0, 2000000, mor_dest))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bulk insert will take the same time as COW as this is the first write "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b76f4744d571475da219ebaa3b7e0430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df2.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, BULK_INSERT_OPERATION_OPT_VAL)\n",
    "      .option(BULK_INSERT_PARALLELISM, 3)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)\n",
    "      .option(STORAGE_TYPE_OPT_KEY, \"MERGE_ON_READ\")\n",
    "      .option(COMPACTION_INLINE_OPT_KEY, \"false\")\n",
    "      .option(COMPACTION_MAX_DELTA_COMMITS_OPT_KEY, \"20\")\n",
    "      .mode(\"Overwrite\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the number of files \n",
    "\n",
    "Let us check the contents of S3 path. Bulk insert operation on Copy-On-Write and Merge-On-Read tables is identical in terms of performance. \n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://<your-bucket>/demos/hudi/hudi_mor_trips_table/\n",
    "                           PRE .hoodie/\n",
    "2020-04-28 23:30:20          0 .hoodie_$folder$\n",
    "2020-04-28 23:30:26         93 .hoodie_partition_metadata\n",
    "2020-04-28 23:30:33    4378000 45b1ce07-f9ac-496d-8b03-20af011a0c44-0_1-194-3566_20200428233020.parquet\n",
    "2020-04-28 23:30:34    5048941 932d5e97-c5f0-4c91-a7f6-f65d487a5e2b-0_2-194-3567_20200428233020.parquet\n",
    "2020-04-28 23:30:34    5065824 ea6e8bfa-e70c-4f7e-90ec-37d018fb0acf-0_0-194-3565_20200428233020.parquet\n",
    "```\n",
    "\n",
    "Notice the delta commits \n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://<your-bucket>/demos/hudi/hudi_mor_trips_table/.hoodie/\n",
    "2020-04-28 23:30:21          0 .aux_$folder$\n",
    "2020-04-28 23:30:21          0 .temp_$folder$\n",
    "2020-04-28 23:30:37       1077 20200428233020.clean\n",
    "2020-04-28 23:30:36       4929 20200428233020.deltacommit\n",
    "2020-04-28 23:30:21          0 archived_$folder$\n",
    "2020-04-28 23:30:21        264 hoodie.properties\n",
    "```\n",
    "\n",
    "This is the first commit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us try to upsert some records into MOR table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f608dbf3d88c4c6896801f93e667db2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------+-------------------+\n",
      "|destination|route_id|trip_id|             tstamp|\n",
      "+-----------+--------+-------+-------------------+\n",
      "|  San Diego|       A|1000000|2020-12-16 16:22:59|\n",
      "|  San Diego|       B|1000001|2020-12-16 16:22:59|\n",
      "|  San Diego|       C|1000002|2020-12-16 16:22:59|\n",
      "|  San Diego|       D|1000003|2020-12-16 16:22:59|\n",
      "|  San Diego|       E|1000004|2020-12-16 16:22:59|\n",
      "|  San Diego|       F|1000005|2020-12-16 16:22:59|\n",
      "|  San Diego|       G|1000006|2020-12-16 16:22:59|\n",
      "|  San Diego|       H|1000007|2020-12-16 16:22:59|\n",
      "|  San Diego|       I|1000008|2020-12-16 16:22:59|\n",
      "|  San Diego|       J|1000009|2020-12-16 16:22:59|\n",
      "+-----------+--------+-------+-------------------+"
     ]
    }
   ],
   "source": [
    "upsert_dest = [\"San Diego\", \"San Diego\", \"San Diego\", \"San Diego\", \"San Diego\",\"San Diego\",\"San Diego\",\"San Diego\",\"San Diego\",\"San Diego\"]\n",
    "df3 = create_json_df(spark, get_json_data(1000000, 10, upsert_dest))\n",
    "df3.count()\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbae500699cf4200b0d699b18e387dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df3.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "      .option(UPSERT_PARALLELISM, 20)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HUDI_CLEANER_POLICY, KEEP_LATEST_COMMITS)\n",
    "      .option(HUDI_COMMITS_RETAINED,config[\"commits_to_retain\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)  \n",
    "      .option(STORAGE_TYPE_OPT_KEY, \"MERGE_ON_READ\")\n",
    "      .option(COMPACTION_INLINE_OPT_KEY, \"false\")\n",
    "      .option(COMPACTION_MAX_DELTA_COMMITS_OPT_KEY, \"20\")\n",
    "      .mode(\"Append\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58ed1a8dba424435ab31aeb5c2c3e20c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------+-------------------+\n",
      "|trip_id|route_id|destination  |tstamp             |\n",
      "+-------+--------+-------------+-------------------+\n",
      "|999996 |G       |Washington DC|2020-12-16 16:20:31|\n",
      "|999997 |H       |Philadelphia |2020-12-16 16:20:31|\n",
      "|999998 |I       |Miami        |2020-12-16 16:20:31|\n",
      "|999999 |J       |San Francisco|2020-12-16 16:20:31|\n",
      "|1000000|A       |Seattle      |2020-12-16 16:20:31|\n",
      "|1000001|B       |New York     |2020-12-16 16:20:31|\n",
      "|1000002|C       |New Jersey   |2020-12-16 16:20:31|\n",
      "|1000003|D       |Los Angeles  |2020-12-16 16:20:31|\n",
      "|1000004|E       |Las Vegas    |2020-12-16 16:20:31|\n",
      "|1000005|F       |Tucson       |2020-12-16 16:20:31|\n",
      "|1000006|G       |Washington DC|2020-12-16 16:20:31|\n",
      "|1000007|H       |Philadelphia |2020-12-16 16:20:31|\n",
      "|1000008|I       |Miami        |2020-12-16 16:20:31|\n",
      "|1000009|J       |San Francisco|2020-12-16 16:20:31|\n",
      "|1000010|A       |Seattle      |2020-12-16 16:20:31|\n",
      "+-------+--------+-------------+-------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trip_id, route_id, destination, tstamp from \"+config['table_name']+ \"_ro where trip_id between 999996 and 1000010\").show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets query the real-time table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a72a343fb1a14349a6001824e9d8630a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------+-------------------+\n",
      "|trip_id|route_id|destination  |tstamp             |\n",
      "+-------+--------+-------------+-------------------+\n",
      "|999996 |G       |Washington DC|2020-12-16 16:20:31|\n",
      "|999997 |H       |Philadelphia |2020-12-16 16:20:31|\n",
      "|999998 |I       |Miami        |2020-12-16 16:20:31|\n",
      "|999999 |J       |San Francisco|2020-12-16 16:20:31|\n",
      "|1000000|A       |San Diego    |2020-12-16 16:22:59|\n",
      "|1000001|B       |San Diego    |2020-12-16 16:22:59|\n",
      "|1000002|C       |San Diego    |2020-12-16 16:22:59|\n",
      "|1000003|D       |San Diego    |2020-12-16 16:22:59|\n",
      "|1000004|E       |San Diego    |2020-12-16 16:22:59|\n",
      "|1000005|F       |San Diego    |2020-12-16 16:22:59|\n",
      "|1000006|G       |San Diego    |2020-12-16 16:22:59|\n",
      "|1000007|H       |San Diego    |2020-12-16 16:22:59|\n",
      "|1000008|I       |San Diego    |2020-12-16 16:22:59|\n",
      "|1000009|J       |San Diego    |2020-12-16 16:22:59|\n",
      "|1000010|A       |Seattle      |2020-12-16 16:20:31|\n",
      "+-------+--------+-------------+-------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trip_id, route_id, destination, tstamp from \"+ config['table_name_rt'] + \" where trip_id between 999996 and 1000010\").show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the S3 path again. There is no change in number of Parquet files after upsert operation unlike Copy-On-Write tables. \n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://<your-bucket>/demos/hudi/hudi_mor_trips_table/\n",
    "                           PRE .hoodie/\n",
    "2020-04-28 23:33:22       2071 .ea6e8bfa-e70c-4f7e-90ec-37d018fb0acf-0_20200428233020.log.1_0-227-3837\n",
    "2020-04-28 23:30:20          0 .hoodie_$folder$\n",
    "2020-04-28 23:30:26         93 .hoodie_partition_metadata\n",
    "2020-04-28 23:30:33    4378000 45b1ce07-f9ac-496d-8b03-20af011a0c44-0_1-194-3566_20200428233020.parquet\n",
    "2020-04-28 23:30:34    5048941 932d5e97-c5f0-4c91-a7f6-f65d487a5e2b-0_2-194-3567_20200428233020.parquet\n",
    "2020-04-28 23:30:34    5065824 ea6e8bfa-e70c-4f7e-90ec-37d018fb0acf-0_0-194-3565_20200428233020.parquet\n",
    "\n",
    "```\n",
    "\n",
    "Now, let us compact using Hudi CLI \n",
    "\n",
    "On EMR:\n",
    "\n",
    "```\n",
    "/usr/lib/hudi/cli/bin/hudi-cli.sh\n",
    "\n",
    "hudi->connect --path s3://<your-bucket>/demos/hudi/hudi_mor_trips_table/\n",
    "\n",
    "hudi:hudi_mor_trips_table->compactions show all\n",
    "╔═════════════════════════╤═══════╤═══════════════════════════════╗\n",
    "║ Compaction Instant Time │ State │ Total FileIds to be Compacted ║\n",
    "╠═════════════════════════╧═══════╧═══════════════════════════════╣\n",
    "║ (empty)                                                         ║\n",
    "╚═════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "```\n",
    "\n",
    "Notice there are no compactions for our mor table. Let us manually schedule and run a compaction. \n",
    "\n",
    "```\n",
    "\n",
    "hudi:hudi_mor_trips_table->compaction schedule\n",
    "Compaction successfully completed for 20200428233601\n",
    "\n",
    "\n",
    "hudi:hudi_mor_trips_table->connect --path s3://<your-bucket>/demos/hudi/hudi_mor_trips_table/\n",
    "\n",
    "hudi:hudi_mor_trips_table->compactions show all\n",
    "20/04/28 23:39:04 INFO timeline.HoodieActiveTimeline: Loaded instants java.util.stream.ReferencePipeline$Head@240d2f9d\n",
    "20/04/28 23:39:04 INFO s3n.S3NativeFileSystem: Opening 's3://<Your S3 Bucket Here>/tmp/hudi/hudi_mor_trips_table/.hoodie/.aux/20200428233601.compaction.requested' for reading\n",
    "╔═════════════════════════╤═══════════╤═══════════════════════════════╗\n",
    "║ Compaction Instant Time │ State     │ Total FileIds to be Compacted ║\n",
    "╠═════════════════════════╪═══════════╪═══════════════════════════════╣\n",
    "║ 20200428233601          │ REQUESTED │ 1                             ║\n",
    "╚═════════════════════════╧═══════════╧═══════════════════════════════╝\n",
    "\n",
    "hudi:hudi_mor_trips_table->compaction run --parallelism 10 --sparkMemory 4G --retry 2 --schemaFilePath s3://<your-bucket>/demos/schema.avsc --compactionInstant 20200428233601\n",
    "\n",
    "Compaction successfully completed for 20200428233601\n",
    "\n",
    "```\n",
    "\n",
    "You can also schedule a compaction using \"compaction schedule\" option. \n",
    "\n",
    "Now, if we check the S3 path, we will see the delta commit has been merged into a new file \n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://<your-bucket>/demos/hudi/hudi_mor_trips_table/\n",
    "                           PRE .hoodie/\n",
    "2020-04-28 23:33:22       2071 .ea6e8bfa-e70c-4f7e-90ec-37d018fb0acf-0_20200428233020.log.1_0-227-3837\n",
    "2020-04-28 23:30:20          0 .hoodie_$folder$\n",
    "2020-04-28 23:30:26         93 .hoodie_partition_metadata\n",
    "2020-04-28 23:30:33    4378000 45b1ce07-f9ac-496d-8b03-20af011a0c44-0_1-194-3566_20200428233020.parquet\n",
    "2020-04-28 23:30:34    5048941 932d5e97-c5f0-4c91-a7f6-f65d487a5e2b-0_2-194-3567_20200428233020.parquet\n",
    "2020-04-28 23:41:48    5065205 ea6e8bfa-e70c-4f7e-90ec-37d018fb0acf-0_0-0-0_20200428233601.parquet\n",
    "2020-04-28 23:30:34    5065824 ea6e8bfa-e70c-4f7e-90ec-37d018fb0acf-0_0-194-3565_20200428233020.parquet\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Partitioned Tables\n",
    "\n",
    "Let's do the same thing with Partitioned Tables. For the sake of this demo, we will be making route_id as partition field. You can also have a nested partition structure like yyyy/mm/dd which is more common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe6c1264fbf49788f459096e5bc7558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## CHANGE ME ##\n",
    "config = {\n",
    "    \"table_name\": \"hudi_partitioned_trips_table\",\n",
    "    \"target\": \"s3://<your-bucket>/demos/hudi/hudi_partitioned_trips_table\",\n",
    "    \"primary_key\": \"trip_id\",\n",
    "    \"sort_key\": \"tstamp\",\n",
    "    \"commits_to_retain\": \"2\",\n",
    "    \"partition_keys\" : \"route_id\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e53620cc366d48d3966cfdbcc78a293a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "part_dest = [\"Seattle\", \"New York\", \"New Jersey\", \"Los Angeles\", \"Las Vegas\", \"Tucson\",\"Washington DC\",\"Philadelphia\",\"Miami\",\"San Francisco\"]\n",
    "df1 = create_json_df(spark, get_json_data(0, 2000000, part_dest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the partitionKey column to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "985f1eca2ea246d4b9fbea6c32d372e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  route_id|\n",
      "+----------+\n",
      "|route_id=A|\n",
      "|route_id=B|\n",
      "|route_id=C|\n",
      "|route_id=D|\n",
      "|route_id=E|\n",
      "+----------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat, col, lit\n",
    "\n",
    "hudiTablePartitionKey=\"route_id\"\n",
    "df1 = df1.withColumn(hudiTablePartitionKey,concat(lit(\"route_id=\"),col(\"route_id\")))\n",
    "df1.select(hudiTablePartitionKey).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can now write out the data to S3. Notice that the Hive Partition Extractor class has changed in the statement below:\n",
    "\n",
    "```\n",
    "      .option(HIVE_PARTITION_FIELDS_OPT_KEY, config[\"partition_keys\"])\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,MULTIPART_KEYS_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(PARTITIONPATH_FIELD_OPT_KEY,\"route_id\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ba37acc26224dfaad641fc373775533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df1.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, BULK_INSERT_OPERATION_OPT_VAL)\n",
    "      .option(BULK_INSERT_PARALLELISM, 6)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HIVE_PARTITION_FIELDS_OPT_KEY, config[\"partition_keys\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,MULTIPART_KEYS_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(PARTITIONPATH_FIELD_OPT_KEY,\"route_id\")\n",
    "      .mode(\"Overwrite\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "282092bb3df44866a2e0b343da6a822b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|createtab_stmt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|CREATE EXTERNAL TABLE `hudi_partitioned_trips_table`(`_hoodie_commit_time` STRING, `_hoodie_commit_seqno` STRING, `_hoodie_record_key` STRING, `_hoodie_partition_path` STRING, `_hoodie_file_name` STRING, `destination` STRING, `trip_id` BIGINT, `tstamp` STRING)\n",
      "PARTITIONED BY (`route_id` STRING)\n",
      "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
      "WITH SERDEPROPERTIES (\n",
      "  'serialization.format' = '1'\n",
      ")\n",
      "STORED AS\n",
      "  INPUTFORMAT 'org.apache.hudi.hadoop.HoodieParquetInputFormat'\n",
      "  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\n",
      "LOCATION 's3://aws-data-analytics-demos-xyz/demos/hudi/hudi_partitioned_trips_table'\n",
      "TBLPROPERTIES (\n",
      "  'last_commit_time_sync' = '20200605043134',\n",
      "  'transient_lastDdlTime' = '1591331522'\n",
      ")\n",
      "|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show create table \"+config['table_name']).show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the partitions fields are present in our Hive table. \n",
    "\n",
    "```\n",
    "PARTITIONED BY (`route_id` STRING)\n",
    "```\n",
    "\n",
    "Let's now query the data and group by the the partition columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "910f2f4b68a84d33b580c523b2d0e1af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|route_id|num_trips|\n",
      "+--------+---------+\n",
      "|A       |200000   |\n",
      "|B       |200000   |\n",
      "|C       |200000   |\n",
      "|D       |200000   |\n",
      "|E       |200000   |\n",
      "|F       |200000   |\n",
      "|G       |200000   |\n",
      "|H       |200000   |\n",
      "|I       |200000   |\n",
      "|J       |200000   |\n",
      "+--------+---------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select route_id, count(*) as num_trips from \"+config['table_name']+\" group by route_id order by route_id\").show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the S3 path\n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://<your-bucket>/demos/hudi/hudi_partitioned_trips_table/\n",
    "                           PRE .hoodie/\n",
    "                           PRE route_id=A/\n",
    "                           PRE route_id=B/\n",
    "                           PRE route_id=C/\n",
    "                           PRE route_id=D/\n",
    "                           PRE route_id=E/\n",
    "                           PRE route_id=F/\n",
    "                           PRE route_id=G/\n",
    "                           PRE route_id=H/\n",
    "                           PRE route_id=I/\n",
    "                           PRE route_id=J/\n",
    "2020-04-28 23:42:50          0 .hoodie_$folder$\n",
    "2020-04-28 23:42:55          0 route_id=A_$folder$\n",
    "2020-04-28 23:42:55          0 route_id=B_$folder$\n",
    "2020-04-28 23:42:57          0 route_id=C_$folder$\n",
    "2020-04-28 23:42:55          0 route_id=D_$folder$\n",
    "2020-04-28 23:42:57          0 route_id=E_$folder$\n",
    "2020-04-28 23:42:55          0 route_id=F_$folder$\n",
    "2020-04-28 23:42:55          0 route_id=G_$folder$\n",
    "2020-04-28 23:42:57          0 route_id=H_$folder$\n",
    "2020-04-28 23:42:55          0 route_id=I_$folder$\n",
    "2020-04-28 23:42:58          0 route_id=J_$folder$\n",
    "\n",
    "```\n",
    "\n",
    "Under each partition, there will be partition metadata\n",
    "\n",
    "```\n",
    "\n",
    "$ aws s3 ls s3://<your-bucket>/demos/hudi/hudi_partitioned_trips_table/route_id=A/\n",
    "2020-04-28 23:42:56         93 .hoodie_partition_metadata\n",
    "2020-04-28 23:42:59    1723564 447b86e6-500c-463a-bdac-74abb867efad-0_0-256-4156_20200428234249.parquet\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other operations Upsert etc. behave the same way on Partitioned tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
