{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert, Update and Delete data on S3 Datalake using Apache Hudi and Amazon EMR - Demo\n",
    "\n",
    "## Table of Contents:\n",
    "\n",
    "1. [Overview](#Overview)\n",
    "2. [Copy On Write](#Copy-On-Write)<br>\n",
    "    2.1 [Bulk Insert the Initial Dataset](#Bulk-Insert-the-Initial-Dataset)<br>\n",
    "    2.2 [Batch Upsert some records](#Batch-Upsert-some-records)<br>\n",
    "    2.3 [Deleting Records](#Deleting-Records)<br>\n",
    "3. [Rollback](#Rollback)<br>\n",
    "4. [Time travel with Hudi](#Time-travel-with-Hudi) \n",
    "5. [Merge on Read](#Merge-On-Read)<br>\n",
    "    5.1 [Bulk Insert the Initial Dataset](#Bulk-Insert-the-Initial-Dataset)<br>\n",
    "    5.2 [Batch Upsert some records](#Batch-Upsert-some-records)<br>\n",
    "    5.3 [Compaction for MOR tables](#Compaction-for-MOR-tables) <br>\n",
    "6. [Working with Partitioned Tables](#Working-with-Partitioned-Tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates using PySpark on [Apache Hudi](https://aws.amazon.com/emr/features/hudi/) on Amazon EMR to insert/upsert/delete records to an S3 data lake.\n",
    "\n",
    "Here are some good reference links to read later:\n",
    "\n",
    "* [Apache Hudi concepts](https://hudi.apache.org/concepts.html)\n",
    "* [How Hudi Works](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hudi-how-it-works.html)\n",
    "\n",
    "This notebook covers the following concepts when writing Copy-On-Write and Merge-On-Read tables to an S3 Datalake:\n",
    "\n",
    "- Write Hudi Spark jobs in PySpark.\n",
    "- Bulk Insert the Initial Dataset.\n",
    "- Write a MultiKey Partitioned table as well as a Non-Partitioned table.\n",
    "- Tune the Bulk Insert write performance as per expected number of target files.\n",
    "- Sync the Hudi tables to the Hive/Glue Catalog.\n",
    "- Upsert some records to a Hudi table.\n",
    "- Delete from records from a Hudi table.\n",
    "- Understand how Hudi Commit Retention policy works.\n",
    "- Time travel with Hudi incremental tables using incremental and point-in-time queries\n",
    "- Perform Insert/Upsert/Delete operations for Merge-on-Read table and understand the difference between MOR and COW tables\n",
    "\n",
    "\n",
    "#### This demo runs fine on a 1 node (r5.4xlarge) cluster with EMR release = 5.30 or 5.30.1 or 5.31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by initializing the Spark Session to connect this notebook to our Spark EMR cluster:\n",
    "\n",
    "Note that the files hudi-spark-bundle.jar and spark-avro.jar are copied into HDFS.\n",
    "\n",
    "hadoop fs -copyFromLocal /usr/lib/hudi/hudi-spark-bundle.jar hdfs:///user/hadoop/\n",
    "\n",
    "hadoop fs -copyFromLocal /usr/lib/spark/external/lib/spark-avro.jar hdfs:///user/hadoop/\n",
    "\n",
    "hadoop fs -copyFromLocal /usr/lib/spark/jars/httpclient-4.5.9.jar hdfs:///user/hadoop/"
    
    "Note: If you plan to use EMR Release 5.32, please make sure to include the following jar as well \n"
    "hadoop fs -copyFromLocal /usr/lib/spark/jars/httpcore-4.4.11.jar hdfs:///user/hadoop/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars': 'hdfs:///user/hadoop/httpclient-4.5.9.jar, hdfs:///user/hadoop/hudi-spark-bundle.jar,hdfs:///user/hadoop/spark-avro.jar', 'spark.sql.hive.convertMetastoreParquet': 'false', 'spark.serializer': 'org.apache.spark.serializer.KryoSerializer', 'spark.dynamicAllocation.executorIdleTimeout': 3600}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":  { \n",
    "             \"spark.jars\":\"hdfs:///user/hadoop/httpclient-4.5.9.jar, hdfs:///user/hadoop/hudi-spark-bundle.jar,hdfs:///user/hadoop/spark-avro.jar\",\n",
    "             \"spark.sql.hive.convertMetastoreParquet\":\"false\",     \n",
    "             \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "             \"spark.dynamicAllocation.executorIdleTimeout\": 3600\n",
    "           } \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Following are the configuration items we will use in this demo\n",
    "\n",
    "Make sure to point the target parameter to your S3 bucket (replace <your-bucket> with an S3 bucket name in your AWS account in the same region as your EMR cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0baeaae65245407691ce3a7a6201b07e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>21</td><td>application_1591208168040_0036</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-1-126.ec2.internal:20888/proxy/application_1591208168040_0036/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-1-78.ec2.internal:8042/node/containerlogs/container_1591208168040_0036_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## CHANGE ME ##\n",
    "config = {\n",
    "    \"table_name\": \"hudi_trips_table\",\n",
    "    \"target\": \"s3://<your-bucket>/demos/hudi/hudi_trips_table\",\n",
    "    \"primary_key\": \"trip_id\",\n",
    "    \"sort_key\": \"tstamp\",\n",
    "    \"commits_to_retain\": \"10\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constants for Python to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee0c1b429be94e8b8f954aca08dfbba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# General Constants\n",
    "HUDI_FORMAT = \"org.apache.hudi\"\n",
    "TABLE_NAME = \"hoodie.table.name\"\n",
    "RECORDKEY_FIELD_OPT_KEY = \"hoodie.datasource.write.recordkey.field\"\n",
    "PRECOMBINE_FIELD_OPT_KEY = \"hoodie.datasource.write.precombine.field\"\n",
    "OPERATION_OPT_KEY = \"hoodie.datasource.write.operation\"\n",
    "BULK_INSERT_OPERATION_OPT_VAL = \"bulk_insert\"\n",
    "UPSERT_OPERATION_OPT_VAL = \"upsert\"\n",
    "BULK_INSERT_PARALLELISM = \"hoodie.bulkinsert.shuffle.parallelism\"\n",
    "UPSERT_PARALLELISM = \"hoodie.upsert.shuffle.parallelism\"\n",
    "S3_CONSISTENCY_CHECK = \"hoodie.consistency.check.enabled\"\n",
    "HUDI_CLEANER_POLICY = \"hoodie.cleaner.policy\"\n",
    "KEEP_LATEST_COMMITS = \"KEEP_LATEST_COMMITS\"\n",
    "HUDI_COMMITS_RETAINED = \"hoodie.cleaner.commits.retained\"\n",
    "PAYLOAD_CLASS_OPT_KEY = \"hoodie.datasource.write.payload.class\"\n",
    "EMPTY_PAYLOAD_CLASS_OPT_VAL = \"org.apache.hudi.common.model.EmptyHoodieRecordPayload\"\n",
    "\n",
    "# Hive Constants\n",
    "HIVE_SYNC_ENABLED_OPT_KEY=\"hoodie.datasource.hive_sync.enable\"\n",
    "HIVE_PARTITION_FIELDS_OPT_KEY=\"hoodie.datasource.hive_sync.partition_fields\"\n",
    "HIVE_ASSUME_DATE_PARTITION_OPT_KEY=\"hoodie.datasource.hive_sync.assume_date_partitioning\"\n",
    "HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY=\"hoodie.datasource.hive_sync.partition_extractor_class\"\n",
    "HIVE_TABLE_OPT_KEY=\"hoodie.datasource.hive_sync.table\"\n",
    "\n",
    "# Partition Constants\n",
    "NONPARTITION_EXTRACTOR_CLASS_OPT_VAL=\"org.apache.hudi.hive.NonPartitionedExtractor\"\n",
    "MULTIPART_KEYS_EXTRACTOR_CLASS_OPT_VAL=\"org.apache.hudi.hive.MultiPartKeysValueExtractor\"\n",
    "KEYGENERATOR_CLASS_OPT_KEY=\"hoodie.datasource.write.keygenerator.class\"\n",
    "NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL=\"org.apache.hudi.keygen.NonpartitionedKeyGenerator\"\n",
    "COMPLEX_KEYGENERATOR_CLASS_OPT_VAL=\"org.apache.hudi.ComplexKeyGenerator\"\n",
    "PARTITIONPATH_FIELD_OPT_KEY=\"hoodie.datasource.write.partitionpath.field\"\n",
    "\n",
    "#Incremental Constants\n",
    "VIEW_TYPE_OPT_KEY=\"hoodie.datasource.view.type\"\n",
    "BEGIN_INSTANTTIME_OPT_KEY=\"hoodie.datasource.read.begin.instanttime\"\n",
    "VIEW_TYPE_INCREMENTAL_OPT_VAL=\"incremental\"\n",
    "END_INSTANTTIME_OPT_KEY=\"hoodie.datasource.read.end.instanttime\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to create JSON data and Spark dataframe from this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b4592019e1486e9558a1e6db81d3c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Generates Data\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def get_json_data(start, count, dest):\n",
    "    time_stamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    data = [{\"trip_id\": i, \"tstamp\": time_stamp, \"route_id\": chr(65 + (i % 10)), \"destination\": dest[i%10]} for i in range(start, start + count)]\n",
    "    return data\n",
    "\n",
    "# Creates the Dataframe\n",
    "def create_json_df(spark, data):\n",
    "    sc = spark.sparkContext\n",
    "    return spark.read.json(sc.parallelize(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulk Insert the Initial Dataset\n",
    "\n",
    "Let's generate 2M records to load into our Data Lake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7d48140c99e46509829d267c4ffa9ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000\n",
      "+-------------+--------+-------+-------------------+\n",
      "|  destination|route_id|trip_id|             tstamp|\n",
      "+-------------+--------+-------+-------------------+\n",
      "|      Seattle|       A|      0|2020-06-05 03:55:01|\n",
      "|     New York|       B|      1|2020-06-05 03:55:01|\n",
      "|   New Jersey|       C|      2|2020-06-05 03:55:01|\n",
      "|  Los Angeles|       D|      3|2020-06-05 03:55:01|\n",
      "|    Las Vegas|       E|      4|2020-06-05 03:55:01|\n",
      "|       Tucson|       F|      5|2020-06-05 03:55:01|\n",
      "|Washington DC|       G|      6|2020-06-05 03:55:01|\n",
      "| Philadelphia|       H|      7|2020-06-05 03:55:01|\n",
      "|        Miami|       I|      8|2020-06-05 03:55:01|\n",
      "|San Francisco|       J|      9|2020-06-05 03:55:01|\n",
      "|      Seattle|       A|     10|2020-06-05 03:55:01|\n",
      "|     New York|       B|     11|2020-06-05 03:55:01|\n",
      "|   New Jersey|       C|     12|2020-06-05 03:55:01|\n",
      "|  Los Angeles|       D|     13|2020-06-05 03:55:01|\n",
      "|    Las Vegas|       E|     14|2020-06-05 03:55:01|\n",
      "|       Tucson|       F|     15|2020-06-05 03:55:01|\n",
      "|Washington DC|       G|     16|2020-06-05 03:55:01|\n",
      "| Philadelphia|       H|     17|2020-06-05 03:55:01|\n",
      "|        Miami|       I|     18|2020-06-05 03:55:01|\n",
      "|San Francisco|       J|     19|2020-06-05 03:55:01|\n",
      "+-------------+--------+-------+-------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "dest = [\"Seattle\", \"New York\", \"New Jersey\", \"Los Angeles\", \"Las Vegas\", \"Tucson\",\"Washington DC\",\"Philadelphia\",\"Miami\",\"San Francisco\"]\n",
    "df1 = create_json_df(spark, get_json_data(0, 2000000, dest))\n",
    "print(df1.count())\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And write the data to S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8989b6a496d54a6e892c27af1dc617c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df1.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, BULK_INSERT_OPERATION_OPT_VAL)\n",
    "      .option(BULK_INSERT_PARALLELISM, 3)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)\n",
    "      .mode(\"Overwrite\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe the number of files in S3. Expected number of files is 3 files as BULK_INSERT_PARALLELISM is set to 3. \n",
    "```\n",
    "\n",
    "$ aws s3 ls s3://<your-bucket>/demos/hudi/hudi_trips_table/ --summarize --human-readable\n",
    "                           PRE .hoodie/\n",
    "2020-04-28 23:11:39    0 Bytes .hoodie_$folder$\n",
    "2020-04-28 23:11:52   93 Bytes .hoodie_partition_metadata\n",
    "2020-04-28 23:11:59    4.8 MiB 0eeafaf2-0110-4056-b509-724b11a4c0b6-0_0-7-67_20200428231141.parquet\n",
    "2020-04-28 23:11:59    4.4 MiB 1ce9cbac-56e4-477e-a2bc-33fe62b3550f-0_1-7-68_20200428231141.parquet\n",
    "2020-04-28 23:11:59    4.6 MiB 579443dc-8f56-4990-bed6-a527f21e9682-0_2-7-69_20200428231141.parquet\n",
    "\n",
    "Total Objects: 5\n",
    "   Total Size: 13.8 MiB\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "Let's inspect the table created and query the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a179ea3e2127445bbd9cb37133d09e8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------+-----------+\n",
      "|database|tableName            |isTemporary|\n",
      "+--------+---------------------+-----------+\n",
      "|default |all_rides            |false      |\n",
      "|default |cloudfront_logs      |false      |\n",
      "|default |elb_logs_hudi_cow    |false      |\n",
      "|default |elb_logs_hudi_cow2   |false      |\n",
      "|default |elb_logs_hudi_cow3   |false      |\n",
      "|default |flight_delays_dl_pq  |false      |\n",
      "|default |flight_delays_pq     |false      |\n",
      "|default |hudi_trips_table     |false      |\n",
      "|default |inputtable           |false      |\n",
      "|default |nyctaxidatayellow    |false      |\n",
      "|default |profile_test_cow     |false      |\n",
      "|default |tbl_data_eng         |false      |\n",
      "|default |tbl_data_eng_demo    |false      |\n",
      "|default |tbl_data_sci         |false      |\n",
      "|default |values__tmp__table__1|false      |\n",
      "|default |yellow_taxi_data     |false      |\n",
      "|default |yellow_taxi_data_test|false      |\n",
      "+--------+---------------------+-----------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "510c7df0c18b4a83a31753fa0b49d741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|createtab_stmt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|CREATE EXTERNAL TABLE `hudi_trips_table`(`_hoodie_commit_time` STRING, `_hoodie_commit_seqno` STRING, `_hoodie_record_key` STRING, `_hoodie_partition_path` STRING, `_hoodie_file_name` STRING, `destination` STRING, `route_id` STRING, `trip_id` BIGINT, `tstamp` STRING)\n",
      "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
      "WITH SERDEPROPERTIES (\n",
      "  'serialization.format' = '1'\n",
      ")\n",
      "STORED AS\n",
      "  INPUTFORMAT 'org.apache.hudi.hadoop.HoodieParquetInputFormat'\n",
      "  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\n",
      "LOCATION 's3://aws-data-analytics-demos-xyz/demos/hudi/hudi_trips_table'\n",
      "TBLPROPERTIES (\n",
      "  'transient_lastDdlTime' = '1591329365',\n",
      "  'last_commit_time_sync' = '20200605035525'\n",
      ")\n",
      "|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show create table \"+config['table_name']).show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the extra columns that are added by Hudi to keep track of commits and filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f840bb90f4448e099dba38d54b05f29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+---------------------------------------------------------------------+-------------+--------+-------+-------------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|_hoodie_file_name                                                    |destination  |route_id|trip_id|tstamp             |\n",
      "+-------------------+--------------------+------------------+----------------------+---------------------------------------------------------------------+-------------+--------+-------+-------------------+\n",
      "|20200605035525     |20200605035525_2_1  |411100            |                      |abf5458e-4a61-49b8-9b6e-508633fb147d-0_2-8-245_20200605035525.parquet|Seattle      |A       |411100 |2020-06-05 03:55:01|\n",
      "|20200605035525     |20200605035525_2_2  |411101            |                      |abf5458e-4a61-49b8-9b6e-508633fb147d-0_2-8-245_20200605035525.parquet|New York     |B       |411101 |2020-06-05 03:55:01|\n",
      "|20200605035525     |20200605035525_2_3  |411102            |                      |abf5458e-4a61-49b8-9b6e-508633fb147d-0_2-8-245_20200605035525.parquet|New Jersey   |C       |411102 |2020-06-05 03:55:01|\n",
      "|20200605035525     |20200605035525_2_4  |411103            |                      |abf5458e-4a61-49b8-9b6e-508633fb147d-0_2-8-245_20200605035525.parquet|Los Angeles  |D       |411103 |2020-06-05 03:55:01|\n",
      "|20200605035525     |20200605035525_2_5  |411104            |                      |abf5458e-4a61-49b8-9b6e-508633fb147d-0_2-8-245_20200605035525.parquet|Las Vegas    |E       |411104 |2020-06-05 03:55:01|\n",
      "|20200605035525     |20200605035525_2_6  |411105            |                      |abf5458e-4a61-49b8-9b6e-508633fb147d-0_2-8-245_20200605035525.parquet|Tucson       |F       |411105 |2020-06-05 03:55:01|\n",
      "|20200605035525     |20200605035525_2_7  |411106            |                      |abf5458e-4a61-49b8-9b6e-508633fb147d-0_2-8-245_20200605035525.parquet|Washington DC|G       |411106 |2020-06-05 03:55:01|\n",
      "|20200605035525     |20200605035525_2_8  |411107            |                      |abf5458e-4a61-49b8-9b6e-508633fb147d-0_2-8-245_20200605035525.parquet|Philadelphia |H       |411107 |2020-06-05 03:55:01|\n",
      "|20200605035525     |20200605035525_2_9  |411108            |                      |abf5458e-4a61-49b8-9b6e-508633fb147d-0_2-8-245_20200605035525.parquet|Miami        |I       |411108 |2020-06-05 03:55:01|\n",
      "|20200605035525     |20200605035525_2_10 |411109            |                      |abf5458e-4a61-49b8-9b6e-508633fb147d-0_2-8-245_20200605035525.parquet|San Francisco|J       |411109 |2020-06-05 03:55:01|\n",
      "|20200605035525     |20200605035525_2_11 |41111             |                      |abf5458e-4a61-49b8-9b6e-508633fb147d-0_2-8-245_20200605035525.parquet|New York     |B       |41111  |2020-06-05 03:55:01|\n",
      "|20200605035525     |20200605035525_2_12 |411110            |                      |abf5458e-4a61-49b8-9b6e-508633fb147d-0_2-8-245_20200605035525.parquet|Seattle      |A       |411110 |2020-06-05 03:55:01|\n",
      "|20200605035525     |20200605035525_2_13 |411111            |                      |abf5458e-4a61-49b8-9b6e-508633fb147d-0_2-8-245_20200605035525.parquet|New York     |B       |411111 |2020-06-05 03:55:01|\n",
      "|20200605035525     |20200605035525_2_14 |411112            |                      |abf5458e-4a61-49b8-9b6e-508633fb147d-0_2-8-245_20200605035525.parquet|New Jersey   |C       |411112 |2020-06-05 03:55:01|\n",
      "|20200605035525     |20200605035525_2_15 |411113            |                      |abf5458e-4a61-49b8-9b6e-508633fb147d-0_2-8-245_20200605035525.parquet|Los Angeles  |D       |411113 |2020-06-05 03:55:01|\n",
      "|20200605035525     |20200605035525_2_16 |411114            |                      |abf5458e-4a61-49b8-9b6e-508633fb147d-0_2-8-245_20200605035525.parquet|Las Vegas    |E       |411114 |2020-06-05 03:55:01|\n",
      "|20200605035525     |20200605035525_2_17 |411115            |                      |abf5458e-4a61-49b8-9b6e-508633fb147d-0_2-8-245_20200605035525.parquet|Tucson       |F       |411115 |2020-06-05 03:55:01|\n",
      "|20200605035525     |20200605035525_2_18 |411116            |                      |abf5458e-4a61-49b8-9b6e-508633fb147d-0_2-8-245_20200605035525.parquet|Washington DC|G       |411116 |2020-06-05 03:55:01|\n",
      "|20200605035525     |20200605035525_2_19 |411117            |                      |abf5458e-4a61-49b8-9b6e-508633fb147d-0_2-8-245_20200605035525.parquet|Philadelphia |H       |411117 |2020-06-05 03:55:01|\n",
      "|20200605035525     |20200605035525_2_20 |411118            |                      |abf5458e-4a61-49b8-9b6e-508633fb147d-0_2-8-245_20200605035525.parquet|Miami        |I       |411118 |2020-06-05 03:55:01|\n",
      "+-------------------+--------------------+------------------+----------------------+---------------------------------------------------------------------+-------------+--------+-------+-------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "df2=spark.read.format(HUDI_FORMAT).load(config[\"target\"]+\"/*\")\n",
    "df2.count()\n",
    "df2.show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can query the Hive table as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de6bfc8c7cf94aba8a032bcda588af14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|2000000 |\n",
      "+--------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from \"+config['table_name']).show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Upsert some records\n",
    "\n",
    "Let's modify a few records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a454f7c02364774833aea04063c0ab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------+-------------------+\n",
      "|trip_id|route_id|destination  |tstamp             |\n",
      "+-------+--------+-------------+-------------------+\n",
      "|1000000|A       |Seattle      |2020-06-05 03:55:01|\n",
      "|1000001|B       |New York     |2020-06-05 03:55:01|\n",
      "|1000002|C       |New Jersey   |2020-06-05 03:55:01|\n",
      "|1000003|D       |Los Angeles  |2020-06-05 03:55:01|\n",
      "|1000004|E       |Las Vegas    |2020-06-05 03:55:01|\n",
      "|1000005|F       |Tucson       |2020-06-05 03:55:01|\n",
      "|1000006|G       |Washington DC|2020-06-05 03:55:01|\n",
      "|1000007|H       |Philadelphia |2020-06-05 03:55:01|\n",
      "|1000008|I       |Miami        |2020-06-05 03:55:01|\n",
      "|1000009|J       |San Francisco|2020-06-05 03:55:01|\n",
      "+-------+--------+-------------+-------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trip_id, route_id, destination, tstamp from \"+config['table_name'] +\" where trip_id between 1000000 and 1000009\").show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1272d886db3481ba7d4dfc4e2148e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10"
     ]
    }
   ],
   "source": [
    "upsert_dest = [\"Boston\", \"Boston\", \"Boston\", \"Boston\", \"Boston\",\"Boston\",\"Boston\",\"Boston\",\"Boston\",\"Boston\"]\n",
    "df3 = create_json_df(spark, get_json_data(1000000, 10, upsert_dest))\n",
    "df3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6615475c159a4e478666aedd69108240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------------+-----------+\n",
      "|trip_id|route_id|             tstamp|destination|\n",
      "+-------+--------+-------------------+-----------+\n",
      "|1000000|       A|2020-06-05 03:58:54|     Boston|\n",
      "|1000001|       B|2020-06-05 03:58:54|     Boston|\n",
      "|1000002|       C|2020-06-05 03:58:54|     Boston|\n",
      "|1000003|       D|2020-06-05 03:58:54|     Boston|\n",
      "|1000004|       E|2020-06-05 03:58:54|     Boston|\n",
      "|1000005|       F|2020-06-05 03:58:54|     Boston|\n",
      "|1000006|       G|2020-06-05 03:58:54|     Boston|\n",
      "|1000007|       H|2020-06-05 03:58:54|     Boston|\n",
      "|1000008|       I|2020-06-05 03:58:54|     Boston|\n",
      "|1000009|       J|2020-06-05 03:58:54|     Boston|\n",
      "+-------+--------+-------------------+-----------+"
     ]
    }
   ],
   "source": [
    "df3.select(\"trip_id\",\"route_id\",\"tstamp\",\"destination\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have changed the destination and timestamp for trip IDs 1000000 to 1000010. Now, let's upsert the changes to S3. Note that the operation now is \"Upsert\" as opposed to BulkInsert for the initial load:\n",
    "\n",
    "```\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6150233265734cdb94f8a81263226c33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df3.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "      .option(UPSERT_PARALLELISM, 20)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HUDI_CLEANER_POLICY, KEEP_LATEST_COMMITS)\n",
    "      .option(HUDI_COMMITS_RETAINED,config[\"commits_to_retain\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)  \n",
    "      .mode(\"Append\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2224660ab4244c38965af520c7f69bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------------+-------------+\n",
      "|trip_id|route_id|tstamp             |destination  |\n",
      "+-------+--------+-------------------+-------------+\n",
      "|999996 |G       |2020-06-05 03:55:01|Washington DC|\n",
      "|999997 |H       |2020-06-05 03:55:01|Philadelphia |\n",
      "|999998 |I       |2020-06-05 03:55:01|Miami        |\n",
      "|999999 |J       |2020-06-05 03:55:01|San Francisco|\n",
      "|1000000|A       |2020-06-05 03:58:54|Boston       |\n",
      "|1000001|B       |2020-06-05 03:58:54|Boston       |\n",
      "|1000002|C       |2020-06-05 03:58:54|Boston       |\n",
      "|1000003|D       |2020-06-05 03:58:54|Boston       |\n",
      "|1000004|E       |2020-06-05 03:58:54|Boston       |\n",
      "|1000005|F       |2020-06-05 03:58:54|Boston       |\n",
      "|1000006|G       |2020-06-05 03:58:54|Boston       |\n",
      "|1000007|H       |2020-06-05 03:58:54|Boston       |\n",
      "|1000008|I       |2020-06-05 03:58:54|Boston       |\n",
      "|1000009|J       |2020-06-05 03:58:54|Boston       |\n",
      "|1000010|A       |2020-06-05 03:55:01|Seattle      |\n",
      "|1000011|B       |2020-06-05 03:55:01|New York     |\n",
      "|1000012|C       |2020-06-05 03:55:01|New Jersey   |\n",
      "|1000013|D       |2020-06-05 03:55:01|Los Angeles  |\n",
      "+-------+--------+-------------------+-------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trip_id, route_id, tstamp, destination from \"+config['table_name'] +\" where trip_id between 999996 and 1000013\").show(50,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check the commit\n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://<your-bucket>/demos/hudi/hudi_trips_table/ --summarize --human-readable\n",
    "                           PRE .hoodie/\n",
    "2020-04-28 23:11:39    0 Bytes .hoodie_$folder$\n",
    "2020-04-28 23:11:52   93 Bytes .hoodie_partition_metadata\n",
    "2020-04-28 23:15:43    4.8 MiB 0eeafaf2-0110-4056-b509-724b11a4c0b6-0_0-51-609_20200428231528.parquet\n",
    "2020-04-28 23:11:59    4.8 MiB 0eeafaf2-0110-4056-b509-724b11a4c0b6-0_0-7-67_20200428231141.parquet\n",
    "2020-04-28 23:11:59    4.4 MiB 1ce9cbac-56e4-477e-a2bc-33fe62b3550f-0_1-7-68_20200428231141.parquet\n",
    "2020-04-28 23:11:59    4.6 MiB 579443dc-8f56-4990-bed6-a527f21e9682-0_2-7-69_20200428231141.parquet\n",
    "\n",
    "Total Objects: 6\n",
    "   Total Size: 18.6 MiB\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we upserted some records, let us try to insert 10 new records into the table. We will use same upsert option. As primary keys 2000000 to 2000009 do not exist in the table, the records will be inserted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce316250b2c24fab97fc0b81ea03e27a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------+-------------------+\n",
      "|destination|route_id|trip_id|             tstamp|\n",
      "+-----------+--------+-------+-------------------+\n",
      "|   Syracuse|       A|2000000|2020-06-05 04:00:34|\n",
      "|   Syracuse|       B|2000001|2020-06-05 04:00:34|\n",
      "|   Syracuse|       C|2000002|2020-06-05 04:00:34|\n",
      "|   Syracuse|       D|2000003|2020-06-05 04:00:34|\n",
      "|   Syracuse|       E|2000004|2020-06-05 04:00:34|\n",
      "|   Syracuse|       F|2000005|2020-06-05 04:00:34|\n",
      "|   Syracuse|       G|2000006|2020-06-05 04:00:34|\n",
      "|   Syracuse|       H|2000007|2020-06-05 04:00:34|\n",
      "|   Syracuse|       I|2000008|2020-06-05 04:00:34|\n",
      "|   Syracuse|       J|2000009|2020-06-05 04:00:34|\n",
      "+-----------+--------+-------+-------------------+"
     ]
    }
   ],
   "source": [
    "insert_dest = [\"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\"]\n",
    "df5 = create_json_df(spark, get_json_data(2000000, 10, insert_dest))\n",
    "df5.count()\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d5d4beaa01c41fcb66e99f3db945c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df5.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "      .option(UPSERT_PARALLELISM, 20)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HUDI_CLEANER_POLICY, KEEP_LATEST_COMMITS)\n",
    "      .option(HUDI_COMMITS_RETAINED,config[\"commits_to_retain\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)  \n",
    "      .mode(\"Append\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f38563c9bf284325bc042eb7d8ab3cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000010"
     ]
    }
   ],
   "source": [
    "df6=spark.read.format(HUDI_FORMAT).load(config[\"target\"]+\"/*\")\n",
    "df6.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b336834b7e41c99f7b7e30ca0bdea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------------+-------------+\n",
      "|trip_id|route_id|tstamp             |destination  |\n",
      "+-------+--------+-------------------+-------------+\n",
      "|2000009|J       |2020-06-05 04:00:34|Syracuse     |\n",
      "|2000008|I       |2020-06-05 04:00:34|Syracuse     |\n",
      "|2000007|H       |2020-06-05 04:00:34|Syracuse     |\n",
      "|2000006|G       |2020-06-05 04:00:34|Syracuse     |\n",
      "|2000001|B       |2020-06-05 04:00:34|Syracuse     |\n",
      "|2000000|A       |2020-06-05 04:00:34|Syracuse     |\n",
      "|2000005|F       |2020-06-05 04:00:34|Syracuse     |\n",
      "|2000004|E       |2020-06-05 04:00:34|Syracuse     |\n",
      "|2000003|D       |2020-06-05 04:00:34|Syracuse     |\n",
      "|2000002|C       |2020-06-05 04:00:34|Syracuse     |\n",
      "|1999997|H       |2020-06-05 03:55:01|Philadelphia |\n",
      "|1999998|I       |2020-06-05 03:55:01|Miami        |\n",
      "|1999999|J       |2020-06-05 03:55:01|San Francisco|\n",
      "+-------+--------+-------------------+-------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trip_id, route_id, tstamp, destination from \"+config['table_name'] +\" where trip_id > 1999996\").show(50,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the records are updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting Records.\n",
    "\n",
    "Apache Hudi supports implementing two types of deletes on data stored in Hudi datasets, by enabling the user to specify a different record payload implementation.\n",
    "\n",
    "* **Soft Deletes** : With soft deletes, user wants to retain the key but just null out the values for all other fields. This can be simply achieved by ensuring the appropriate fields are nullable in the dataset schema and simply upserting the dataset after setting these fields to null.\n",
    "    \n",
    "* **Hard Deletes** : A stronger form of delete is to physically remove any trace of the record from the dataset. \n",
    "\n",
    "Let's now execute some hard delete operations on our dataset which will remove the records from our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's delete the 10 records with the \"Syracuse\" destination we added to the table. Note that the only change is the single line that set the hoodie.datasource.write.payload.class to org.apache.hudi.EmptyHoodieRecordPayload to delete the records.\n",
    "\n",
    "```\n",
    ".option(PAYLOAD_CLASS_OPT_KEY, EMPTY_PAYLOAD_CLASS_OPT_VAL)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90c03a10c82044c0a3a4ebca52c9931d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------+-------------------+\n",
      "|destination|route_id|trip_id|             tstamp|\n",
      "+-----------+--------+-------+-------------------+\n",
      "|   Syracuse|       A|2000000|2020-06-05 04:00:34|\n",
      "|   Syracuse|       B|2000001|2020-06-05 04:00:34|\n",
      "|   Syracuse|       C|2000002|2020-06-05 04:00:34|\n",
      "|   Syracuse|       D|2000003|2020-06-05 04:00:34|\n",
      "|   Syracuse|       E|2000004|2020-06-05 04:00:34|\n",
      "|   Syracuse|       F|2000005|2020-06-05 04:00:34|\n",
      "|   Syracuse|       G|2000006|2020-06-05 04:00:34|\n",
      "|   Syracuse|       H|2000007|2020-06-05 04:00:34|\n",
      "|   Syracuse|       I|2000008|2020-06-05 04:00:34|\n",
      "|   Syracuse|       J|2000009|2020-06-05 04:00:34|\n",
      "+-----------+--------+-------+-------------------+"
     ]
    }
   ],
   "source": [
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "663bbc64ac0f495d9ec5cd5484a6c49f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df5.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "      .option(UPSERT_PARALLELISM, 20)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HUDI_CLEANER_POLICY, KEEP_LATEST_COMMITS)\n",
    "      .option(HUDI_COMMITS_RETAINED,config[\"commits_to_retain\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)\n",
    "      .option(PAYLOAD_CLASS_OPT_KEY, EMPTY_PAYLOAD_CLASS_OPT_VAL)\n",
    "      .mode(\"Append\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38aa0b5eb87343dca425efc44b75460f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------------+-------------+\n",
      "|trip_id|route_id|tstamp             |destination  |\n",
      "+-------+--------+-------------------+-------------+\n",
      "|1999997|H       |2020-06-05 03:55:01|Philadelphia |\n",
      "|1999998|I       |2020-06-05 03:55:01|Miami        |\n",
      "|1999999|J       |2020-06-05 03:55:01|San Francisco|\n",
      "+-------+--------+-------------------+-------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trip_id, route_id, tstamp, destination from \"+config['table_name'] +\" where trip_id > 1999996\").show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the records > 2000000 no longer exist in our table.\n",
    "\n",
    "Let's observe the number of files in S3. Expected : 6 files (initial files (3) + one upsert + one insert + one delete = 6)\n",
    "\n",
    "```\n",
    "\n",
    "$ aws s3 ls s3://<your-bucket>/demos/hudi/hudi_trips_table/ --summarize --human-readable\n",
    "                           PRE .hoodie/\n",
    "2020-04-28 23:11:39    0 Bytes .hoodie_$folder$\n",
    "2020-04-28 23:11:52   93 Bytes .hoodie_partition_metadata\n",
    "2020-04-28 23:15:43    4.8 MiB 0eeafaf2-0110-4056-b509-724b11a4c0b6-0_0-51-609_20200428231528.parquet\n",
    "2020-04-28 23:11:59    4.8 MiB 0eeafaf2-0110-4056-b509-724b11a4c0b6-0_0-7-67_20200428231141.parquet\n",
    "2020-04-28 23:18:24    4.4 MiB 1ce9cbac-56e4-477e-a2bc-33fe62b3550f-0_0-135-1376_20200428231814.parquet\n",
    "2020-04-28 23:17:17    4.4 MiB 1ce9cbac-56e4-477e-a2bc-33fe62b3550f-0_0-93-1007_20200428231705.parquet\n",
    "2020-04-28 23:11:59    4.4 MiB 1ce9cbac-56e4-477e-a2bc-33fe62b3550f-0_1-7-68_20200428231141.parquet\n",
    "2020-04-28 23:11:59    4.6 MiB 579443dc-8f56-4990-bed6-a527f21e9682-0_2-7-69_20200428231141.parquet\n",
    "\n",
    "Total Objects: 8\n",
    "   Total Size: 27.5 MiB\n",
    "\n",
    "```\n",
    "\n",
    "In our example, we set number of commits to retain as 10. So, maximum only 10 new files can be created on top of our bulk insert files. i.e., 13 files in total. If we had set the commits_to_retain as 2, the number of files created will not increase beyond initial files(3) + commits_to_retain(2) = 5 files. This is because Hudi Cleaning Policy will delete older files when writing based on the commit retain policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rollback\n",
    "\n",
    "Let's say we want to roll back the last delete we made. \n",
    "\n",
    "/usr/lib/hudi/cli/bin/hudi-cli.sh\n",
    "\n",
    "connect --path s3://<your-bucket>/demos/hudi/hudi_trips_table/\n",
    "\n",
    "commits show\n",
    "\n",
    "hudi:hudi_trips_table->commits show\n",
    "20/04/28 23:46:09 INFO s3n.S3NativeFileSystem: Opening 's3://<Your S3 Bucket Here>/tmp/hudi/hudi_trips_table/.hoodie/20200428231705.commit' for reading\n",
    "20/04/28 23:46:09 INFO s3n.S3NativeFileSystem: Opening 's3://<Your S3 Bucket Here>/tmp/hudi/hudi_trips_table/.hoodie/20200428231528.commit' for reading\n",
    "20/04/28 23:46:09 INFO s3n.S3NativeFileSystem: Opening 's3://<Your S3 Bucket Here>/tmp/hudi/hudi_trips_table/.hoodie/20200428231141.commit' for reading\n",
    "╔════════════════╤═════════════════════╤═══════════════════╤═════════════════════╤══════════════════════════╤═══════════════════════╤══════════════════════════════╤══════════════╗\n",
    "║ CommitTime     │ Total Bytes Written │ Total Files Added │ Total Files Updated │ Total Partitions Written │ Total Records Written │ Total Update Records Written │ Total Errors ║\n",
    "╠════════════════╪═════════════════════╪═══════════════════╪═════════════════════╪══════════════════════════╪═══════════════════════╪══════════════════════════════╪══════════════╣\n",
    "║ 20200428231814 │ 4.4 MB              │ 0                 │ 1                   │ 1                        │ 642132                │ 0                            │ 0            ║\n",
    "╟────────────────┼─────────────────────┼───────────────────┼─────────────────────┼──────────────────────────┼───────────────────────┼──────────────────────────────┼──────────────╢\n",
    "║ 20200428231528 │ 4.8 MB              │ 0                 │ 1                   │ 1                        │ 697329                │ 10                           │ 0            ║\n",
    "╟────────────────┼─────────────────────┼───────────────────┼─────────────────────┼──────────────────────────┼───────────────────────┼──────────────────────────────┼──────────────╢\n",
    "║ 20200428231528 │ 4.8 MB              │ 0                 │ 1                   │ 1                        │ 697329                │ 10                           │ 0            ║\n",
    "╟────────────────┼─────────────────────┼───────────────────┼─────────────────────┼──────────────────────────┼───────────────────────┼──────────────────────────────┼──────────────╢\n",
    "\n",
    "║ 20200428231141 │ 13.8 MB             │ 3                 │ 0                   │ 1                        │ 2000000               │ 0                            │ 0            ║\n",
    "╚════════════════╧═════════════════════╧═══════════════════╧═════════════════════╧══════════════════════════╧═══════════════════════╧══════════════════════════════╧══════════════╝\n",
    "\n",
    "commit rollback --commit 20200428231814\n",
    "    \n",
    "Now let us check what happened to the records we deleted earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d3e1367ac494f79a129399dc79ba6eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------------+-------------+\n",
      "|trip_id|route_id|tstamp             |destination  |\n",
      "+-------+--------+-------------------+-------------+\n",
      "|2000009|J       |2020-06-05 04:00:34|Syracuse     |\n",
      "|2000008|I       |2020-06-05 04:00:34|Syracuse     |\n",
      "|2000007|H       |2020-06-05 04:00:34|Syracuse     |\n",
      "|2000006|G       |2020-06-05 04:00:34|Syracuse     |\n",
      "|2000001|B       |2020-06-05 04:00:34|Syracuse     |\n",
      "|2000000|A       |2020-06-05 04:00:34|Syracuse     |\n",
      "|2000005|F       |2020-06-05 04:00:34|Syracuse     |\n",
      "|2000004|E       |2020-06-05 04:00:34|Syracuse     |\n",
      "|2000003|D       |2020-06-05 04:00:34|Syracuse     |\n",
      "|2000002|C       |2020-06-05 04:00:34|Syracuse     |\n",
      "|1999997|H       |2020-06-05 03:55:01|Philadelphia |\n",
      "|1999998|I       |2020-06-05 03:55:01|Miami        |\n",
      "|1999999|J       |2020-06-05 03:55:01|San Francisco|\n",
      "+-------+--------+-------------------+-------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trip_id, route_id, tstamp, destination from \"+config['table_name'] +\" where trip_id > 1999996\").show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Travel with Hudi \n",
    "\n",
    "Now, let us time travel with Hudi (query previous commits) with incremental and point-in-time queries\n",
    "\n",
    "First, lets check out incremental queries\n",
    "\n",
    "In EMR cluster, let us check Hudi CLI and check how commits look right now \n",
    "\n",
    "/usr/lib/hudi/cli/bin/hudi-cli.sh\n",
    "\n",
    "connect --path s3://<your-bucket>/demos/hudi/hudi_trips_table/\n",
    "\n",
    "commits show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe9aa7788eee4061bc1d4ec5de4c7eb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|commitTime    |\n",
      "+--------------+\n",
      "|20200605035525|\n",
      "|20200605035916|\n",
      "|20200605040050|\n",
      "+--------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select distinct(_hoodie_commit_time) as commitTime from hudi_trips_table order by commitTime\").show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5457617c342436ab5f9b3adb4df9842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commits: \n",
      "Row(commitTime='20200605035525')\n",
      "Row(commitTime='20200605035916')\n",
      "Row(commitTime='20200605040050')\n",
      "begin time: 20200605035525\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----------+--------+-------+-------------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|destination|route_id|trip_id|             tstamp|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----------+--------+-------+-------------------+\n",
      "|     20200605035916|20200605035916_0_...|           1000000|                      |b4fc34bd-52ba-452...|     Boston|       A|1000000|2020-06-05 03:58:54|\n",
      "|     20200605035916|20200605035916_0_...|           1000001|                      |b4fc34bd-52ba-452...|     Boston|       B|1000001|2020-06-05 03:58:54|\n",
      "|     20200605035916|20200605035916_0_...|           1000002|                      |b4fc34bd-52ba-452...|     Boston|       C|1000002|2020-06-05 03:58:54|\n",
      "|     20200605035916|20200605035916_0_...|           1000003|                      |b4fc34bd-52ba-452...|     Boston|       D|1000003|2020-06-05 03:58:54|\n",
      "|     20200605035916|20200605035916_0_...|           1000004|                      |b4fc34bd-52ba-452...|     Boston|       E|1000004|2020-06-05 03:58:54|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----------+--------+-------+-------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "commits = spark.sql(\"select distinct(_hoodie_commit_time) as commitTime from  hudi_trips_table order by commitTime\").collect()\n",
    "print(\"commits: \")\n",
    "for elem in commits: print (elem) \n",
    "\n",
    "beginTime = commits[-3][0] # commit time we are interested in\n",
    "print(\"begin time: \" + beginTime)\n",
    "\n",
    "# incrementally query data\n",
    "incViewDF = spark.read.format(\"org.apache.hudi\") \\\n",
    "                   .option(VIEW_TYPE_OPT_KEY, VIEW_TYPE_INCREMENTAL_OPT_VAL) \\\n",
    "                   .option(BEGIN_INSTANTTIME_OPT_KEY, beginTime) \\\n",
    "                   .load(config[\"target\"]) \n",
    "\n",
    "incViewDF.show(5)\n",
    "incViewDF.registerTempTable(\"hudi_incr_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5761607c730b4897a62d0b8a01983e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+--------+-----------+-------------------+\n",
      "|_hoodie_commit_time|trip_id|route_id|destination|             tstamp|\n",
      "+-------------------+-------+--------+-----------+-------------------+\n",
      "|     20200605035916|1000000|       A|     Boston|2020-06-05 03:58:54|\n",
      "|     20200605035916|1000001|       B|     Boston|2020-06-05 03:58:54|\n",
      "|     20200605035916|1000002|       C|     Boston|2020-06-05 03:58:54|\n",
      "|     20200605035916|1000003|       D|     Boston|2020-06-05 03:58:54|\n",
      "|     20200605035916|1000004|       E|     Boston|2020-06-05 03:58:54|\n",
      "|     20200605035916|1000005|       F|     Boston|2020-06-05 03:58:54|\n",
      "|     20200605035916|1000006|       G|     Boston|2020-06-05 03:58:54|\n",
      "|     20200605035916|1000007|       H|     Boston|2020-06-05 03:58:54|\n",
      "|     20200605035916|1000008|       I|     Boston|2020-06-05 03:58:54|\n",
      "|     20200605035916|1000009|       J|     Boston|2020-06-05 03:58:54|\n",
      "+-------------------+-------+--------+-----------+-------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select `_hoodie_commit_time`, trip_id, route_id, destination, tstamp from  hudi_incr_table where trip_id between 999996 and 1000013\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets check out point-in-time queries. i.e., query from a specific commit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d8648e320dc44f489a79c3c7a94c1f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commits: \n",
      "Row(commitTime='20200605035525')\n",
      "Row(commitTime='20200605035916')\n",
      "Row(commitTime='20200605040050')"
     ]
    }
   ],
   "source": [
    "commits = spark.sql(\"select distinct(_hoodie_commit_time) as commitTime from  hudi_trips_table order by commitTime\").collect()\n",
    "print(\"commits: \")\n",
    "for elem in commits: print (elem) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "425fadb07bad48c7955f621462b1442e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end time: 20200605035525\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----------+--------+-------+-------------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|destination|route_id|trip_id|             tstamp|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----------+--------+-------+-------------------+\n",
      "|     20200605035525|  20200605035525_0_1|                 0|                      |b4fc34bd-52ba-452...|    Seattle|       A|      0|2020-06-05 03:55:01|\n",
      "|     20200605035525|  20200605035525_0_2|                 1|                      |b4fc34bd-52ba-452...|   New York|       B|      1|2020-06-05 03:55:01|\n",
      "|     20200605035525|  20200605035525_0_3|                10|                      |b4fc34bd-52ba-452...|    Seattle|       A|     10|2020-06-05 03:55:01|\n",
      "|     20200605035525|  20200605035525_0_4|               100|                      |b4fc34bd-52ba-452...|    Seattle|       A|    100|2020-06-05 03:55:01|\n",
      "|     20200605035525|  20200605035525_0_5|              1000|                      |b4fc34bd-52ba-452...|    Seattle|       A|   1000|2020-06-05 03:55:01|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----------+--------+-------+-------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "#Lets check out the first state. i.e., before we updated the trip to Boston\n",
    "\n",
    "beginTime = \"000\" #Represents all commits > this time.\n",
    "endTime = commits[-3][0] # first commit\n",
    "print(\"end time: \"+endTime)\n",
    "\n",
    "# incrementally query data\n",
    "incViewDF = spark.read.format(\"org.apache.hudi\") \\\n",
    "                   .option(VIEW_TYPE_OPT_KEY, VIEW_TYPE_INCREMENTAL_OPT_VAL) \\\n",
    "                   .option(BEGIN_INSTANTTIME_OPT_KEY, beginTime) \\\n",
    "                   .option(END_INSTANTTIME_OPT_KEY, endTime) \\\n",
    "                   .load(config[\"target\"]) \n",
    "\n",
    "incViewDF.show(5)\n",
    "incViewDF.registerTempTable(\"hudi_incr_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2caa14c914214038ad67046041e2aca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+--------+-------------+-------------------+\n",
      "|_hoodie_commit_time|trip_id|route_id|  destination|             tstamp|\n",
      "+-------------------+-------+--------+-------------+-------------------+\n",
      "|     20200605035525|1000000|       A|      Seattle|2020-06-05 03:55:01|\n",
      "|     20200605035525|1000001|       B|     New York|2020-06-05 03:55:01|\n",
      "|     20200605035525|1000002|       C|   New Jersey|2020-06-05 03:55:01|\n",
      "|     20200605035525|1000003|       D|  Los Angeles|2020-06-05 03:55:01|\n",
      "|     20200605035525|1000004|       E|    Las Vegas|2020-06-05 03:55:01|\n",
      "|     20200605035525|1000005|       F|       Tucson|2020-06-05 03:55:01|\n",
      "|     20200605035525|1000006|       G|Washington DC|2020-06-05 03:55:01|\n",
      "|     20200605035525|1000007|       H| Philadelphia|2020-06-05 03:55:01|\n",
      "|     20200605035525|1000008|       I|        Miami|2020-06-05 03:55:01|\n",
      "|     20200605035525|1000009|       J|San Francisco|2020-06-05 03:55:01|\n",
      "|     20200605035525|1000010|       A|      Seattle|2020-06-05 03:55:01|\n",
      "|     20200605035525|1000011|       B|     New York|2020-06-05 03:55:01|\n",
      "|     20200605035525|1000012|       C|   New Jersey|2020-06-05 03:55:01|\n",
      "|     20200605035525|1000013|       D|  Los Angeles|2020-06-05 03:55:01|\n",
      "|     20200605035525| 999996|       G|Washington DC|2020-06-05 03:55:01|\n",
      "|     20200605035525| 999997|       H| Philadelphia|2020-06-05 03:55:01|\n",
      "|     20200605035525| 999998|       I|        Miami|2020-06-05 03:55:01|\n",
      "|     20200605035525| 999999|       J|San Francisco|2020-06-05 03:55:01|\n",
      "+-------------------+-------+--------+-------------+-------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select `_hoodie_commit_time`, trip_id, route_id, destination, tstamp from  hudi_incr_table where trip_id between 999996 and 1000013\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that the records from 1000000 to 1000009 display first state of our table before we upserted the trip destination to Boston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge On Read \n",
    "\n",
    "The default table type is Copy-On-Write which is best suited for read-heavy workloads with modest writes. Copy-On-Write creates commit files with original data + the new changes during writing itself. While this increases latency on writes, this set up makes it more manageable for faster read.\n",
    "\n",
    "For near real-time applications that mandate quick upserts, MERGE_ON_READ table type would be better suited. MOR table stores incoming upserts for each file group, onto a row based delta log (In Avro file format). This log is then merged with the existing Parquet file using a a compactor during reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "737248ee577f4056bdc1f340eefc26e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## CHANGE ME ##\n",
    "config = {\n",
    "    \"table_name\": \"hudi_mor_trips_table\",\n",
    "    \"table_name_rt\": \"hudi_mor_trips_table_rt\",\n",
    "    \"target\": \"s3://<your-bucket>/demos/hudi/hudi_mor_trips_table\",\n",
    "    \"primary_key\": \"trip_id\",\n",
    "    \"sort_key\": \"tstamp\",\n",
    "    \"commits_to_retain\": \"2\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b94b994b47045c0a0113bbe6b0f53d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "STORAGE_TYPE_OPT_KEY=\"hoodie.datasource.write.storage.type\"\n",
    "COMPACTION_INLINE_OPT_KEY=\"hoodie.compact.inline\"\n",
    "COMPACTION_MAX_DELTA_COMMITS_OPT_KEY=\"hoodie.compact.inline.max.delta.commits\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a00d3e3011546ab9436abf6bfb2a313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+-------+-------------------+\n",
      "|  destination|route_id|trip_id|             tstamp|\n",
      "+-------------+--------+-------+-------------------+\n",
      "|      Seattle|       A|      0|2020-06-05 04:20:33|\n",
      "|     New York|       B|      1|2020-06-05 04:20:33|\n",
      "|   New Jersey|       C|      2|2020-06-05 04:20:33|\n",
      "|  Los Angeles|       D|      3|2020-06-05 04:20:33|\n",
      "|    Las Vegas|       E|      4|2020-06-05 04:20:33|\n",
      "|       Tucson|       F|      5|2020-06-05 04:20:33|\n",
      "|Washington DC|       G|      6|2020-06-05 04:20:33|\n",
      "| Philadelphia|       H|      7|2020-06-05 04:20:33|\n",
      "|        Miami|       I|      8|2020-06-05 04:20:33|\n",
      "|San Francisco|       J|      9|2020-06-05 04:20:33|\n",
      "|      Seattle|       A|     10|2020-06-05 04:20:33|\n",
      "|     New York|       B|     11|2020-06-05 04:20:33|\n",
      "|   New Jersey|       C|     12|2020-06-05 04:20:33|\n",
      "|  Los Angeles|       D|     13|2020-06-05 04:20:33|\n",
      "|    Las Vegas|       E|     14|2020-06-05 04:20:33|\n",
      "|       Tucson|       F|     15|2020-06-05 04:20:33|\n",
      "|Washington DC|       G|     16|2020-06-05 04:20:33|\n",
      "| Philadelphia|       H|     17|2020-06-05 04:20:33|\n",
      "|        Miami|       I|     18|2020-06-05 04:20:33|\n",
      "|San Francisco|       J|     19|2020-06-05 04:20:33|\n",
      "+-------------+--------+-------+-------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "mor_dest = [\"Seattle\", \"New York\", \"New Jersey\", \"Los Angeles\", \"Las Vegas\", \"Tucson\",\"Washington DC\",\"Philadelphia\",\"Miami\",\"San Francisco\"]\n",
    "df2 = create_json_df(spark, get_json_data(0, 2000000, mor_dest))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bulk insert will take the same time as COW as this is the first write "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf153852716f48a58772beecad709570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df2.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, BULK_INSERT_OPERATION_OPT_VAL)\n",
    "      .option(BULK_INSERT_PARALLELISM, 3)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)\n",
    "      .option(STORAGE_TYPE_OPT_KEY, \"MERGE_ON_READ\")\n",
    "      .option(COMPACTION_INLINE_OPT_KEY, \"false\")\n",
    "      .option(COMPACTION_MAX_DELTA_COMMITS_OPT_KEY, \"20\")\n",
    "      .mode(\"Overwrite\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the number of files \n",
    "\n",
    "Let us check the contents of S3 path. Bulk insert operation on Copy-On-Write and Merge-On-Read tables is identical in terms of performance. \n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://<your-bucket>/demos/hudi/hudi_mor_trips_table/\n",
    "                           PRE .hoodie/\n",
    "2020-04-28 23:30:20          0 .hoodie_$folder$\n",
    "2020-04-28 23:30:26         93 .hoodie_partition_metadata\n",
    "2020-04-28 23:30:33    4378000 45b1ce07-f9ac-496d-8b03-20af011a0c44-0_1-194-3566_20200428233020.parquet\n",
    "2020-04-28 23:30:34    5048941 932d5e97-c5f0-4c91-a7f6-f65d487a5e2b-0_2-194-3567_20200428233020.parquet\n",
    "2020-04-28 23:30:34    5065824 ea6e8bfa-e70c-4f7e-90ec-37d018fb0acf-0_0-194-3565_20200428233020.parquet\n",
    "```\n",
    "\n",
    "Notice the delta commits \n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://<your-bucket>/demos/hudi/hudi_mor_trips_table/.hoodie/\n",
    "2020-04-28 23:30:21          0 .aux_$folder$\n",
    "2020-04-28 23:30:21          0 .temp_$folder$\n",
    "2020-04-28 23:30:37       1077 20200428233020.clean\n",
    "2020-04-28 23:30:36       4929 20200428233020.deltacommit\n",
    "2020-04-28 23:30:21          0 archived_$folder$\n",
    "2020-04-28 23:30:21        264 hoodie.properties\n",
    "```\n",
    "\n",
    "This is the first commit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us try to upsert some records into MOR table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e58a29ccc6554be2beae4fe88e518c96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------+-------------------+\n",
      "|destination|route_id|trip_id|             tstamp|\n",
      "+-----------+--------+-------+-------------------+\n",
      "|  San Diego|       A|1000000|2020-06-05 04:22:12|\n",
      "|  San Diego|       B|1000001|2020-06-05 04:22:12|\n",
      "|  San Diego|       C|1000002|2020-06-05 04:22:12|\n",
      "|  San Diego|       D|1000003|2020-06-05 04:22:12|\n",
      "|  San Diego|       E|1000004|2020-06-05 04:22:12|\n",
      "|  San Diego|       F|1000005|2020-06-05 04:22:12|\n",
      "|  San Diego|       G|1000006|2020-06-05 04:22:12|\n",
      "|  San Diego|       H|1000007|2020-06-05 04:22:12|\n",
      "|  San Diego|       I|1000008|2020-06-05 04:22:12|\n",
      "|  San Diego|       J|1000009|2020-06-05 04:22:12|\n",
      "+-----------+--------+-------+-------------------+"
     ]
    }
   ],
   "source": [
    "upsert_dest = [\"San Diego\", \"San Diego\", \"San Diego\", \"San Diego\", \"San Diego\",\"San Diego\",\"San Diego\",\"San Diego\",\"San Diego\",\"San Diego\"]\n",
    "df3 = create_json_df(spark, get_json_data(1000000, 10, upsert_dest))\n",
    "df3.count()\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f001ef40be524a0f90398ea04ec11ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df3.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "      .option(UPSERT_PARALLELISM, 20)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HUDI_CLEANER_POLICY, KEEP_LATEST_COMMITS)\n",
    "      .option(HUDI_COMMITS_RETAINED,config[\"commits_to_retain\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)  \n",
    "      .option(STORAGE_TYPE_OPT_KEY, \"MERGE_ON_READ\")\n",
    "      .option(COMPACTION_INLINE_OPT_KEY, \"false\")\n",
    "      .option(COMPACTION_MAX_DELTA_COMMITS_OPT_KEY, \"20\")\n",
    "      .mode(\"Append\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec2e6408952e4665a3650ee635df6045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------+-------------------+\n",
      "|trip_id|route_id|destination  |tstamp             |\n",
      "+-------+--------+-------------+-------------------+\n",
      "|1000000|A       |Seattle      |2020-06-05 04:20:33|\n",
      "|1000001|B       |New York     |2020-06-05 04:20:33|\n",
      "|1000002|C       |New Jersey   |2020-06-05 04:20:33|\n",
      "|1000003|D       |Los Angeles  |2020-06-05 04:20:33|\n",
      "|1000004|E       |Las Vegas    |2020-06-05 04:20:33|\n",
      "|1000005|F       |Tucson       |2020-06-05 04:20:33|\n",
      "|1000006|G       |Washington DC|2020-06-05 04:20:33|\n",
      "|1000007|H       |Philadelphia |2020-06-05 04:20:33|\n",
      "|1000008|I       |Miami        |2020-06-05 04:20:33|\n",
      "|1000009|J       |San Francisco|2020-06-05 04:20:33|\n",
      "|1000010|A       |Seattle      |2020-06-05 04:20:33|\n",
      "|999996 |G       |Washington DC|2020-06-05 04:20:33|\n",
      "|999997 |H       |Philadelphia |2020-06-05 04:20:33|\n",
      "|999998 |I       |Miami        |2020-06-05 04:20:33|\n",
      "|999999 |J       |San Francisco|2020-06-05 04:20:33|\n",
      "+-------+--------+-------------+-------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trip_id, route_id, destination, tstamp from \"+config['table_name']+ \"_ro where trip_id between 999996 and 1000010\").show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets query the real-time table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f849a1f6eb4c9b82fa7a1adc54afa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------+-------------------+\n",
      "|trip_id|route_id|destination  |tstamp             |\n",
      "+-------+--------+-------------+-------------------+\n",
      "|1000000|A       |San Diego    |2020-06-05 04:22:12|\n",
      "|1000001|B       |San Diego    |2020-06-05 04:22:12|\n",
      "|1000002|C       |San Diego    |2020-06-05 04:22:12|\n",
      "|1000003|D       |San Diego    |2020-06-05 04:22:12|\n",
      "|1000004|E       |San Diego    |2020-06-05 04:22:12|\n",
      "|1000005|F       |San Diego    |2020-06-05 04:22:12|\n",
      "|1000006|G       |San Diego    |2020-06-05 04:22:12|\n",
      "|1000007|H       |San Diego    |2020-06-05 04:22:12|\n",
      "|1000008|I       |San Diego    |2020-06-05 04:22:12|\n",
      "|1000009|J       |San Diego    |2020-06-05 04:22:12|\n",
      "|1000010|A       |Seattle      |2020-06-05 04:20:33|\n",
      "|999996 |G       |Washington DC|2020-06-05 04:20:33|\n",
      "|999997 |H       |Philadelphia |2020-06-05 04:20:33|\n",
      "|999998 |I       |Miami        |2020-06-05 04:20:33|\n",
      "|999999 |J       |San Francisco|2020-06-05 04:20:33|\n",
      "+-------+--------+-------------+-------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trip_id, route_id, destination, tstamp from \"+ config['table_name_rt'] + \" where trip_id between 999996 and 1000010\").show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the S3 path again. There is no change in number of Parquet files after upsert operation unlike Copy-On-Write tables. \n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://<your-bucket>/demos/hudi/hudi_mor_trips_table/\n",
    "                           PRE .hoodie/\n",
    "2020-04-28 23:33:22       2071 .ea6e8bfa-e70c-4f7e-90ec-37d018fb0acf-0_20200428233020.log.1_0-227-3837\n",
    "2020-04-28 23:30:20          0 .hoodie_$folder$\n",
    "2020-04-28 23:30:26         93 .hoodie_partition_metadata\n",
    "2020-04-28 23:30:33    4378000 45b1ce07-f9ac-496d-8b03-20af011a0c44-0_1-194-3566_20200428233020.parquet\n",
    "2020-04-28 23:30:34    5048941 932d5e97-c5f0-4c91-a7f6-f65d487a5e2b-0_2-194-3567_20200428233020.parquet\n",
    "2020-04-28 23:30:34    5065824 ea6e8bfa-e70c-4f7e-90ec-37d018fb0acf-0_0-194-3565_20200428233020.parquet\n",
    "\n",
    "```\n",
    "\n",
    "Now, let us compact using Hudi CLI \n",
    "\n",
    "On EMR:\n",
    "\n",
    "```\n",
    "/usr/lib/hudi/cli/bin/hudi-cli.sh\n",
    "\n",
    "hudi->connect --path s3://<your-bucket>/demos/hudi/hudi_mor_trips_table/\n",
    "\n",
    "hudi:hudi_mor_trips_table->compactions show all\n",
    "╔═════════════════════════╤═══════╤═══════════════════════════════╗\n",
    "║ Compaction Instant Time │ State │ Total FileIds to be Compacted ║\n",
    "╠═════════════════════════╧═══════╧═══════════════════════════════╣\n",
    "║ (empty)                                                         ║\n",
    "╚═════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "```\n",
    "\n",
    "Notice there are no compactions for our mor table. Let us manually schedule and run a compaction. \n",
    "\n",
    "```\n",
    "\n",
    "hudi:hudi_mor_trips_table->compaction schedule\n",
    "Compaction successfully completed for 20200428233601\n",
    "\n",
    "\n",
    "hudi:hudi_mor_trips_table->connect --path s3://<your-bucket>/demos/hudi/hudi_mor_trips_table/\n",
    "\n",
    "hudi:hudi_mor_trips_table->compactions show all\n",
    "20/04/28 23:39:04 INFO timeline.HoodieActiveTimeline: Loaded instants java.util.stream.ReferencePipeline$Head@240d2f9d\n",
    "20/04/28 23:39:04 INFO s3n.S3NativeFileSystem: Opening 's3://<Your S3 Bucket Here>/tmp/hudi/hudi_mor_trips_table/.hoodie/.aux/20200428233601.compaction.requested' for reading\n",
    "╔═════════════════════════╤═══════════╤═══════════════════════════════╗\n",
    "║ Compaction Instant Time │ State     │ Total FileIds to be Compacted ║\n",
    "╠═════════════════════════╪═══════════╪═══════════════════════════════╣\n",
    "║ 20200428233601          │ REQUESTED │ 1                             ║\n",
    "╚═════════════════════════╧═══════════╧═══════════════════════════════╝\n",
    "\n",
    "hudi:hudi_mor_trips_table->compaction run --parallelism 10 --sparkMemory 4G --retry 2 --schemaFilePath s3://<your-bucket>/demos/schema.avsc --compactionInstant 20200428233601\n",
    "\n",
    "Compaction successfully completed for 20200428233601\n",
    "\n",
    "```\n",
    "\n",
    "You can also schedule a compaction using \"compaction schedule\" option. \n",
    "\n",
    "Now, if we check the S3 path, we will see the delta commit has been merged into a new file \n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://<your-bucket>/demos/hudi/hudi_mor_trips_table/\n",
    "                           PRE .hoodie/\n",
    "2020-04-28 23:33:22       2071 .ea6e8bfa-e70c-4f7e-90ec-37d018fb0acf-0_20200428233020.log.1_0-227-3837\n",
    "2020-04-28 23:30:20          0 .hoodie_$folder$\n",
    "2020-04-28 23:30:26         93 .hoodie_partition_metadata\n",
    "2020-04-28 23:30:33    4378000 45b1ce07-f9ac-496d-8b03-20af011a0c44-0_1-194-3566_20200428233020.parquet\n",
    "2020-04-28 23:30:34    5048941 932d5e97-c5f0-4c91-a7f6-f65d487a5e2b-0_2-194-3567_20200428233020.parquet\n",
    "2020-04-28 23:41:48    5065205 ea6e8bfa-e70c-4f7e-90ec-37d018fb0acf-0_0-0-0_20200428233601.parquet\n",
    "2020-04-28 23:30:34    5065824 ea6e8bfa-e70c-4f7e-90ec-37d018fb0acf-0_0-194-3565_20200428233020.parquet\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Partitioned Tables\n",
    "\n",
    "Let's do the same thing with Partitioned Tables. For the sake of this demo, we will be making route_id as partition field. You can also have a nested partition structure like yyyy/mm/dd which is more common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe6c1264fbf49788f459096e5bc7558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## CHANGE ME ##\n",
    "config = {\n",
    "    \"table_name\": \"hudi_partitioned_trips_table\",\n",
    "    \"target\": \"s3://<your-bucket>/demos/hudi/hudi_partitioned_trips_table\",\n",
    "    \"primary_key\": \"trip_id\",\n",
    "    \"sort_key\": \"tstamp\",\n",
    "    \"commits_to_retain\": \"2\",\n",
    "    \"partition_keys\" : \"route_id\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e53620cc366d48d3966cfdbcc78a293a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "part_dest = [\"Seattle\", \"New York\", \"New Jersey\", \"Los Angeles\", \"Las Vegas\", \"Tucson\",\"Washington DC\",\"Philadelphia\",\"Miami\",\"San Francisco\"]\n",
    "df1 = create_json_df(spark, get_json_data(0, 2000000, part_dest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the partitionKey column to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "985f1eca2ea246d4b9fbea6c32d372e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  route_id|\n",
      "+----------+\n",
      "|route_id=A|\n",
      "|route_id=B|\n",
      "|route_id=C|\n",
      "|route_id=D|\n",
      "|route_id=E|\n",
      "+----------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat, col, lit\n",
    "\n",
    "hudiTablePartitionKey=\"route_id\"\n",
    "df1 = df1.withColumn(hudiTablePartitionKey,concat(lit(\"route_id=\"),col(\"route_id\")))\n",
    "df1.select(hudiTablePartitionKey).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can now write out the data to S3. Notice that the Hive Partition Extractor class has changed in the statement below:\n",
    "\n",
    "```\n",
    "      .option(HIVE_PARTITION_FIELDS_OPT_KEY, config[\"partition_keys\"])\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,MULTIPART_KEYS_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(PARTITIONPATH_FIELD_OPT_KEY,\"route_id\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ba37acc26224dfaad641fc373775533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df1.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, BULK_INSERT_OPERATION_OPT_VAL)\n",
    "      .option(BULK_INSERT_PARALLELISM, 6)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HIVE_PARTITION_FIELDS_OPT_KEY, config[\"partition_keys\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,MULTIPART_KEYS_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(PARTITIONPATH_FIELD_OPT_KEY,\"route_id\")\n",
    "      .mode(\"Overwrite\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "282092bb3df44866a2e0b343da6a822b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|createtab_stmt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|CREATE EXTERNAL TABLE `hudi_partitioned_trips_table`(`_hoodie_commit_time` STRING, `_hoodie_commit_seqno` STRING, `_hoodie_record_key` STRING, `_hoodie_partition_path` STRING, `_hoodie_file_name` STRING, `destination` STRING, `trip_id` BIGINT, `tstamp` STRING)\n",
      "PARTITIONED BY (`route_id` STRING)\n",
      "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
      "WITH SERDEPROPERTIES (\n",
      "  'serialization.format' = '1'\n",
      ")\n",
      "STORED AS\n",
      "  INPUTFORMAT 'org.apache.hudi.hadoop.HoodieParquetInputFormat'\n",
      "  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\n",
      "LOCATION 's3://aws-data-analytics-demos-xyz/demos/hudi/hudi_partitioned_trips_table'\n",
      "TBLPROPERTIES (\n",
      "  'last_commit_time_sync' = '20200605043134',\n",
      "  'transient_lastDdlTime' = '1591331522'\n",
      ")\n",
      "|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show create table \"+config['table_name']).show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the partitions fields are present in our Hive table. \n",
    "\n",
    "```\n",
    "PARTITIONED BY (`route_id` STRING)\n",
    "```\n",
    "\n",
    "Let's now query the data and group by the the partition columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "910f2f4b68a84d33b580c523b2d0e1af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|route_id|num_trips|\n",
      "+--------+---------+\n",
      "|A       |200000   |\n",
      "|B       |200000   |\n",
      "|C       |200000   |\n",
      "|D       |200000   |\n",
      "|E       |200000   |\n",
      "|F       |200000   |\n",
      "|G       |200000   |\n",
      "|H       |200000   |\n",
      "|I       |200000   |\n",
      "|J       |200000   |\n",
      "+--------+---------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select route_id, count(*) as num_trips from \"+config['table_name']+\" group by route_id order by route_id\").show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the S3 path\n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://<your-bucket>/demos/hudi/hudi_partitioned_trips_table/\n",
    "                           PRE .hoodie/\n",
    "                           PRE route_id=A/\n",
    "                           PRE route_id=B/\n",
    "                           PRE route_id=C/\n",
    "                           PRE route_id=D/\n",
    "                           PRE route_id=E/\n",
    "                           PRE route_id=F/\n",
    "                           PRE route_id=G/\n",
    "                           PRE route_id=H/\n",
    "                           PRE route_id=I/\n",
    "                           PRE route_id=J/\n",
    "2020-04-28 23:42:50          0 .hoodie_$folder$\n",
    "2020-04-28 23:42:55          0 route_id=A_$folder$\n",
    "2020-04-28 23:42:55          0 route_id=B_$folder$\n",
    "2020-04-28 23:42:57          0 route_id=C_$folder$\n",
    "2020-04-28 23:42:55          0 route_id=D_$folder$\n",
    "2020-04-28 23:42:57          0 route_id=E_$folder$\n",
    "2020-04-28 23:42:55          0 route_id=F_$folder$\n",
    "2020-04-28 23:42:55          0 route_id=G_$folder$\n",
    "2020-04-28 23:42:57          0 route_id=H_$folder$\n",
    "2020-04-28 23:42:55          0 route_id=I_$folder$\n",
    "2020-04-28 23:42:58          0 route_id=J_$folder$\n",
    "\n",
    "```\n",
    "\n",
    "Under each partition, there will be partition metadata\n",
    "\n",
    "```\n",
    "\n",
    "$ aws s3 ls s3://<your-bucket>/demos/hudi/hudi_partitioned_trips_table/route_id=A/\n",
    "2020-04-28 23:42:56         93 .hoodie_partition_metadata\n",
    "2020-04-28 23:42:59    1723564 447b86e6-500c-463a-bdac-74abb867efad-0_0-256-4156_20200428234249.parquet\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other operations Upsert etc. behave the same way on Partitioned tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
